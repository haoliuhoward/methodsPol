<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Class details | Methods of Political Analysis</title>
    <link>/class/</link>
      <atom:link href="/class/index.xml" rel="self" type="application/rss+xml" />
    <description>Class details</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en</language>
    <image>
      <url>/media/social-image.png</url>
      <title>Class details</title>
      <link>/class/</link>
    </image>
    
    <item>
      <title>Bootstrap</title>
      <link>/class/bootstrap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/bootstrap/</guid>
      <description>&lt;h1 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h1&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/slides/code/day2-uncertainty.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day2-uncertainty.R&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;bootstrapping-concepts&#34;&gt;Bootstrapping concepts&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(moderndive)
library(infer)
library(socviz)
library(broom)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember, our estimate is based on a sample from some population, and each sample is going to give us a different estimate. This means our estimates will &lt;strong&gt;vary&lt;/strong&gt; from sample to sample. How can we quantify this &lt;strong&gt;variability&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;One approach is via &lt;strong&gt;bootstrapping&lt;/strong&gt;, where we:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Simulate many new datasets out of our original dataset&lt;/li&gt;
&lt;li&gt;Estimate the thing we want to estimate in each of those &lt;em&gt;bootstrapped&lt;/em&gt; samples&lt;/li&gt;
&lt;li&gt;Look at the distribution of estimates across bootstrap samples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That distribution of bootstrapped estimates gives us a sense for how much an estimate might vary from sample to sample.&lt;/p&gt;
&lt;h2 id=&#34;using-the-infer-package&#34;&gt;Using the &lt;code&gt;infer&lt;/code&gt; package&lt;/h2&gt;
&lt;h3 id=&#34;bootstrapping-averages&#34;&gt;Bootstrapping averages&lt;/h3&gt;
&lt;p&gt;Let’s do this with the &lt;code&gt;infer&lt;/code&gt; package, to estimate the number of kids the average American has.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;%
  # specify the outcome variable
  specify(response = childs) %&amp;gt;%
  # generate the bootstrapped samples
  generate(reps = 1000, type = &amp;quot;bootstrap&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Response: childs (numeric)
## # A tibble: 2,859,000 × 2
## # Groups:   replicate [1,000]
##    replicate childs
##        &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
##  1         1      1
##  2         1      2
##  3         1      2
##  4         1      7
##  5         1      3
##  6         1      7
##  7         1      3
##  8         1      2
##  9         1      6
## 10         1      2
## # ℹ 2,858,990 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how each of the 1,000 bootstrapped samples has the same number of observations as the original dataset.&lt;/p&gt;
&lt;p&gt;The above is equivalent to doing this 1,000 times:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;%
  # subset down to just kids
  select(childs) %&amp;gt;%
  # sample with replacement, same size as original dataset
  sample_n(nrow(gss_sm), replace = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2,867 × 1
##    childs
##     &amp;lt;dbl&amp;gt;
##  1      0
##  2      0
##  3      3
##  4      4
##  5      3
##  6      1
##  7      3
##  8      2
##  9      3
## 10      1
## # ℹ 2,857 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then calculate the average number of kids in each of those 1,000 bootstraps:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_kids = gss_sm %&amp;gt;%
  # specify the outcome variable
  specify(response = childs) %&amp;gt;%
  # generate the bootstrapped samples
  generate(reps = 1000, type = &amp;quot;bootstrap&amp;quot;) %&amp;gt;%
  # find the average # of kids in each bootstrap sample
  calculate(stat = &amp;quot;mean&amp;quot;)

boot_kids
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Response: childs (numeric)
## # A tibble: 1,000 × 2
##    replicate  stat
##        &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1         1  1.80
##  2         2  1.83
##  3         3  1.89
##  4         4  1.84
##  5         5  1.86
##  6         6  1.80
##  7         7  1.79
##  8         8  1.79
##  9         9  1.89
## 10        10  1.85
## # ℹ 990 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;stat&lt;/code&gt; is the average number of kids in each bootstrap (&lt;code&gt;replicate&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We can look at the distribution to get a sense for the variability in the estimates:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(boot_kids, aes(x = stat)) +
  geom_density(color = &amp;quot;white&amp;quot;, fill = &amp;quot;coral&amp;quot;, alpha = .7) +
  theme_bw() +
  labs(title = &amp;quot;Average number of kids across bootstraps&amp;quot;,
       x = NULL, y = NULL) +
  scale_x_continuous(limits = c(1, 3))
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/bootstrap_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Notice how almost all bootstrapped estimates of the average number of kids are between 1.75 and 2, and the vast majority are in a narrower range than that.&lt;/p&gt;
&lt;p&gt;We can also &lt;strong&gt;quantify&lt;/strong&gt; this variation by taking the standard deviation of &lt;code&gt;stat&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_kids %&amp;gt;%
  summarise(se = sd(stat))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 1
##       se
##    &amp;lt;dbl&amp;gt;
## 1 0.0311
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is also known as the &lt;strong&gt;standard error&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;variability-gets-bigger-as-sample-size-gets-smaller&#34;&gt;Variability gets bigger as sample size gets smaller&lt;/h4&gt;
&lt;p&gt;The process above gives us a sense of the variability in our estimate of the average number of kids in the US, based on our survey of &lt;code&gt;\(\approx\)&lt;/code&gt; 2,800 people. What if we had a much smaller survey? Say 100 people?&lt;/p&gt;
&lt;p&gt;We can mimic that below by taking 100 random people from &lt;code&gt;gss_sm&lt;/code&gt; and pretending that’s our full survey:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_kids = gss_sm %&amp;gt;%
  # smaller survey of only 100 people
  sample_n(100) %&amp;gt;%
  # specify the outcome variable
  specify(response = childs) %&amp;gt;%
  # generate the bootstrapped samples
  generate(reps = 1000, type = &amp;quot;bootstrap&amp;quot;) %&amp;gt;%
  # find the average # of kids in each bootstrap sample
  calculate(stat = &amp;quot;mean&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how much wider this distribution is than the one above. The variability in our bootstrapped estimates is much higher!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(boot_kids, aes(x = stat)) +
  geom_density(color = &amp;quot;white&amp;quot;, fill = &amp;quot;coral&amp;quot;, alpha = .7) +
  theme_bw() +
  labs(title = &amp;quot;Average number of kids across bootstraps&amp;quot;,
       x = NULL, y = NULL) +
  scale_x_continuous(limits = c(1, 3))
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/bootstrap_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;You can quantify this too; notice how much bigger the standard error is of this much smaller survey:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_kids %&amp;gt;%
  summarise(se = sd(stat))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 1
##      se
##   &amp;lt;dbl&amp;gt;
## 1 0.166
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;bootstrapping-regression-coefficients&#34;&gt;Bootstrapping regression coefficients&lt;/h3&gt;
&lt;p&gt;We can use bootstrapping to approximate the variability in lots of things we might want to estimaet, including coefficients from regression.&lt;/p&gt;
&lt;p&gt;Say we wanted to look at the effect of age on whether or not someone has children:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lm(childs ~ age, data = gss_sm) %&amp;gt;%
  tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)   0.153    0.0858       1.79 7.40e- 2
## 2 age           0.0345   0.00164     21.0  1.83e-91
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember this estimate comes from a &lt;em&gt;sample&lt;/em&gt;. Other samples will give us different estimates. We can get a sense for how much coefficients will vary across samples with bootstrapping:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_age = gss_sm %&amp;gt;%
  # specify is now Y ~ X
  specify(childs ~ age) %&amp;gt;%
  generate(reps = 1000, type = &amp;quot;bootstrap&amp;quot;) %&amp;gt;%
  # &amp;quot;slope&amp;quot; is another word for coefficient
  calculate(stat = &amp;quot;slope&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how the code looks a bit different: we put &lt;code&gt;childs ~ age&lt;/code&gt; in &lt;code&gt;specify()&lt;/code&gt; and we gotta tell R to calculate the “slope”, or coefficient, from &lt;code&gt;lm(childs ~ age)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can plot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(boot_age, aes(x = stat)) +
  geom_density(color = &amp;quot;white&amp;quot;, fill = &amp;quot;coral&amp;quot;, alpha = .7) +
  theme_bw() +
  labs(title = &amp;quot;Estimate of relationship between age and number of kids&amp;quot;,
       subtitle = &amp;quot;Coefficient estimates across bootstrapped samples.&amp;quot;,
       x = NULL, y = NULL)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/bootstrap_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Notice how coefficient estimates vary a lot, from .03 to almost .04.&lt;/p&gt;
&lt;h2 id=&#34;standard-errors-and-confidence-intervals&#34;&gt;Standard errors and confidence intervals&lt;/h2&gt;
&lt;p&gt;The distributions we get from bootstrapping give us a sense for the &lt;strong&gt;variability&lt;/strong&gt; in our sample estimates. We can quantify that variability in two ways:&lt;/p&gt;
&lt;p&gt;One is through the standard error (the standard deviation of the bootstrapped sample estimates):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_age %&amp;gt;%
  summarise(se = sd(stat))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 1
##        se
##     &amp;lt;dbl&amp;gt;
## 1 0.00158
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The other is the confidence interval, which provides our “best guess” of the thing we are trying to estimate:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_age %&amp;gt;% get_confidence_interval(level = .95)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   lower_ci upper_ci
##      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1   0.0315   0.0377
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The standard is 95%: so the two numbers give us the upper and lower bound of the middle 95% of the bootstrapped distribution.&lt;/p&gt;
&lt;p&gt;Notice that as the confidence increases, the bounds get larger:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_age %&amp;gt;% get_confidence_interval(level = .99)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   lower_ci upper_ci
##      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1   0.0307   0.0390
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the confidence decreases, the bounds get smaller:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_age %&amp;gt;% get_confidence_interval(level = .5)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   lower_ci upper_ci
##      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1   0.0335   0.0356
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hypothesis-testing&#34;&gt;Hypothesis testing&lt;/h2&gt;
&lt;p&gt;Remember, hypothesis testing is about how we decide between two competing hypotheses – or theories about the relationship between two variables – using data.&lt;/p&gt;
&lt;p&gt;Take the example from class, on whether 
&lt;a href=&#34;https://www.theatlantic.com/health/archive/2013/05/study-mens-biceps-predict-their-political-ideologies/275942/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bicep size predicts conservative ideology&lt;/a&gt;. The two competing hypotheses are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Null hypothesis: there is no relationship between biceps and conservative ideology&lt;/li&gt;
&lt;li&gt;Alternative hypothesis: there is a positive relationship between biceps and conservative ideology&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We make up fake data below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(23424)
# fake bicep data
fake = tibble(person = 1:100,
              bicep = rnorm(100),
              conservative = runif(100)*100 + 2*bicep)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We estimate the relationship between bicep and ideology:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# what is the effect?
lm(conservative ~ bicep, data = fake) %&amp;gt;% summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = conservative ~ bicep, data = fake)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -45.688 -24.505  -5.035  28.053  54.741 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   46.839      2.892  16.197   &amp;lt;2e-16 ***
## bicep          3.996      2.754   1.451     0.15    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 28.92 on 98 degrees of freedom
## Multiple R-squared:  0.02103,	Adjusted R-squared:  0.01104 
## F-statistic: 2.105 on 1 and 98 DF,  p-value: 0.15
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember, this estimate is based on one sample. How much might estimates vary? We can use bootstrapping to get a sense:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_bicep = fake %&amp;gt;%
  specify(conservative ~ bicep) %&amp;gt;%
  generate(reps = 1000, type = &amp;quot;bootstrap&amp;quot;) %&amp;gt;%
  calculate(stat = &amp;quot;slope&amp;quot;)


ggplot(boot_bicep, aes(x = stat)) +
  geom_density(color = &amp;quot;white&amp;quot;, fill = &amp;quot;coral&amp;quot;, alpha = .7) +
  theme_bw() +
  labs(title = &amp;quot;Estimate of relationship between bicep size and ideology score&amp;quot;,
       subtitle = &amp;quot;Coefficient estimates across bootstrapped samples.&amp;quot;,
       x = NULL, y = NULL)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/bootstrap_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Notice how much the coefficient can vary: in some cases as large as 10 or more, in others 0, and in some cases even negative.&lt;/p&gt;
&lt;p&gt;How do we decide, based on this distribution, whether there is in fact that a stable relationship between biceps and ideology?&lt;/p&gt;
&lt;h3 id=&#34;hypothesis-testing-permutation&#34;&gt;Hypothesis testing: permutation&lt;/h3&gt;
&lt;p&gt;One way to decide is to simulate what the world might look like under the null hypothesis – a world where biceps don’t affect ideology.&lt;/p&gt;
&lt;p&gt;We can do this by randomly “shuffling” or &lt;em&gt;permuting&lt;/em&gt; the values in our bicep variable. Why? This mimics the idea that if bicep size and ideology are unrelated, you could completely alter one variable without affecting the other. So we:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;“shuffle” or permute the bicep variable&lt;/li&gt;
&lt;li&gt;estimate &lt;code&gt;lm(conservative ~ bicep)&lt;/code&gt;, store coefficient on bicep&lt;/li&gt;
&lt;li&gt;repeat N times&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# permutation based hypothesis testing
null_biceps = fake %&amp;gt;%
  # specify is now Y ~ X
  specify(conservative ~ bicep) %&amp;gt;%
  # define null hypothesis: &amp;quot;independence&amp;quot;
  hypothesize(null = &amp;quot;independence&amp;quot;) %&amp;gt;%
  # now we do &amp;quot;permute&amp;quot; instead of bootstrap
  generate(reps = 1000, type = &amp;quot;permute&amp;quot;) %&amp;gt;%
  # &amp;quot;slope&amp;quot; is another word for coefficient
  calculate(stat = &amp;quot;slope&amp;quot;)

null_biceps
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Response: conservative (numeric)
## Explanatory: bicep (numeric)
## Null Hypothesis: independence
## # A tibble: 1,000 × 2
##    replicate   stat
##        &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
##  1         1  3.60 
##  2         2  3.28 
##  3         3  1.73 
##  4         4  0.390
##  5         5  2.15 
##  6         6  5.20 
##  7         7 -5.17 
##  8         8 -4.43 
##  9         9 -2.34 
## 10        10  1.15 
## # ℹ 990 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row here is the estimated coefficient of bicep from a permuted sample – a sample where the values of bicep have been shuffled. Look at the distribution:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(null_biceps, aes(x = stat)) + geom_histogram(color = &amp;quot;white&amp;quot;,
                                                     fill = &amp;quot;coral&amp;quot;) +
  theme_bw() +
  labs(title = &amp;quot;The Simulated Null Distribution&amp;quot;,
       subtitle = &amp;quot;What we would observe in a world where biceps and ideology are unrelated.&amp;quot;,
       x = &amp;quot;Permuted coefficient estimates from lm(affairs ~ children)&amp;quot;,
       y = NULL)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/bootstrap_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;This is our simulation of what we might observe under the &lt;em&gt;null hypothesis&lt;/em&gt; – that there is no relationship between biceps and ideology. Two key takeaways here:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;As we would expect, most of our simulated coefficient estimates are close to zero. This makes sense! The null hypothesis is precisely that there is no relationship between the two (e.g., that the coefficient is not greater than zero!).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Even in a world where the null is true, you can still get pretty large coefficient estimates by chance, even as large as -10 and +10.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Point (2) is really important: totally by chance, two variables that are unrelated can still have strong positive or negative correlations. It is &lt;em&gt;unlikely that this happens, but it’s still possible&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;With a simulation of what the world would like under the null hypothesis, we can turn back to our original estimate:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lm(conservative ~ bicep, data = fake) %&amp;gt;% moderndive::get_regression_table()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 7
##   term      estimate std_error statistic p_value lower_ci upper_ci
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 intercept    46.8       2.89     16.2     0       41.1     52.6 
## 2 bicep         4.00      2.75      1.45    0.15    -1.47     9.46
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How likely is it that our estimate would have occurred by chance?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(null_biceps, aes(x = stat)) + geom_histogram(color = &amp;quot;white&amp;quot;,
                                                     fill = &amp;quot;coral&amp;quot;) +
  theme_bw() +
  labs(title = &amp;quot;The Simulated Null Distribution&amp;quot;,
       subtitle = &amp;quot;What we would observe in a world where biceps and ideology are unrelated.&amp;quot;,
       x = &amp;quot;Permuted coefficient estimates from lm(affairs ~ children)&amp;quot;,
       y = NULL) +
  geom_vline(xintercept = 3.996, lty = 2, color = &amp;quot;black&amp;quot;, size = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/bootstrap_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Our estimate is a fair bit larger than what you would expect to observe under the null hypothesis. We can calculate exactly how much larger using the &lt;strong&gt;p-value&lt;/strong&gt;. We set &lt;code&gt;direction = right&lt;/code&gt; because we want to know how much &lt;em&gt;larger&lt;/em&gt; the estimate is than expected.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;get_p_value(null_biceps, obs_stat = 3.996, direction = &amp;quot;both&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 1
##   p_value
##     &amp;lt;dbl&amp;gt;
## 1   0.168
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;p-value&lt;/strong&gt; is .084, meaning that only 8.4% of the estimates we got by chance are as large or larger than our original result. In other words, our estimate is larger than 84% of the results we could observe by chance.&lt;/p&gt;
&lt;p&gt;We can calculate this “by hand” too, like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;null_biceps %&amp;gt;%
  mutate(bigger = ifelse(stat &amp;gt; 3.996, &amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;)) %&amp;gt;%
  group_by(bigger) %&amp;gt;%
  tally() %&amp;gt;%
  mutate(percent = n/sum(n))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 3
##   bigger     n percent
##   &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 no       916   0.916
## 2 yes       84   0.084
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;84% larger than we might observe by chance – sounds like our result is unusually large, and unlikely to be the result of random chance. But is it unlikely enough for us to be confident?&lt;/p&gt;
&lt;h3 id=&#34;alpha-levels-standards-of-evidence&#34;&gt;Alpha-levels (standards of evidence)&lt;/h3&gt;
&lt;p&gt;Remember the standard and totally arbitrary &lt;em&gt;alpha-level&lt;/em&gt; accepted in the scientific community is .05. This means that p-values lower than .05 are considered too small to be the result of random chance, while p-values higher than that are considered too large to feel confident in. Arbitrary? Yes.&lt;/p&gt;
&lt;p&gt;Since our p-value of .084 is larger than the .05 standard (the alpha-level), we would &lt;em&gt;fail to reject the null hypothesis&lt;/em&gt;. In other words, the result is too plausible in a world where null hypothesis is true for us to reject it!&lt;/p&gt;
&lt;h3 id=&#34;hypothesis-testing-confidence-intervals&#34;&gt;Hypothesis testing: confidence intervals&lt;/h3&gt;
&lt;p&gt;A different, and much more straightforward way, to do hypothesis testing is to bootstrap confidence intervals and see if they exclude 0. Let’s do the bootstrap.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# boot_bicep = fake %&amp;gt;%
#   specify(conservative ~ bicep) %&amp;gt;%
#   generate(reps = 1000, type = &amp;quot;boot&amp;quot;) %&amp;gt;%
#   calculate(stat = &amp;quot;slope&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the distribution of bootstrapped estimates from our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(boot_bicep, aes(x = stat)) + geom_histogram(color = &amp;quot;white&amp;quot;,
                                                     fill = &amp;quot;coral&amp;quot;) +
  theme_bw() +
  labs(title = &amp;quot;Distribution of bootstrapped estimates&amp;quot;,
       subtitle = &amp;quot;How the relationship between biceps and ideology might vary across samples.&amp;quot;,
       x = &amp;quot;Bootstrapped coefficient estimates from lm(conservative ~ bicep)&amp;quot;,
       y = NULL)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/bootstrap_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Here’s the standard error:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;boot_bicep %&amp;gt;%
  summarise(se = sd(stat))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 1
##      se
##   &amp;lt;dbl&amp;gt;
## 1  2.85
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the 95% confidence interval:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;get_confidence_interval(boot_bicep, level = .95)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   lower_ci upper_ci
##      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1    -1.71     9.55
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice this interval &lt;em&gt;includes&lt;/em&gt; 0. This means that the range of values that are our “best guess” for the effect of biceps on ideology &lt;strong&gt;include&lt;/strong&gt; the possibility that there is no effect (or that the effect is negative). Following the standard of using .05 as the threshold, we would &lt;em&gt;fail to reject the null hypothesis of no effect of biceps on ideology&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Here’s the confidence interval again, with shading:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(boot_bicep, aes(x = stat)) + geom_histogram(color = &amp;quot;white&amp;quot;,
                                                     fill = &amp;quot;coral&amp;quot;) +
  theme_bw() +
  labs(title = &amp;quot;Distribution of bootstrapped estimates&amp;quot;,
       subtitle = &amp;quot;How the relationship between biceps and ideology might vary across samples.&amp;quot;,
       x = &amp;quot;Bootstrapped coefficient estimates from lm(conservative ~ bicep)&amp;quot;,
       y = NULL) +
  annotate(geom = &amp;quot;rect&amp;quot;, xmin = get_confidence_interval(boot_bicep)[[&amp;quot;lower_ci&amp;quot;]],
           xmax = get_confidence_interval(boot_bicep)[[&amp;quot;upper_ci&amp;quot;]], ymin = 0, ymax = Inf,
           fill = &amp;quot;#2ECC40&amp;quot;, alpha = 0.25)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/bootstrap_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;alpha-levels-tradeoffs&#34;&gt;Alpha-levels: tradeoffs&lt;/h2&gt;
&lt;p&gt;Remember, the (abitrary) convention for deciding between competing hypotheses is a p-value less than .05; that .05 threshold is known as the &lt;code&gt;alpha-level&lt;/code&gt;. An important thing to remember is that there is a trade-off in the size of the alpha-level:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the bigger the alpha-level, the lower our standard of evidence for rejecting the null, which means a lower chance of Type 2 error (e.g., letting the guilty go free) and a higher chance of Type 1 error (e.g., convicting the innocent)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the smaller the alpha-level, the higher our standard of evidence for rejecting the null, which means a higher chance of Type 2 error and a lower chance of Type 1 error&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s use simulation to see this. Let’s pretend once again that &lt;code&gt;gss_sm&lt;/code&gt; is the whole of the American public. We want to estimate the effect of age on children in this population. H1 is that people who are older are more likely to have children than people who are younger. H0 (the null) is that people who are older don’t have children at higher rates than people who are younger.&lt;/p&gt;
&lt;p&gt;We can know the exact effect of age on children in this made-up example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lm(childs ~ age, data = gss_sm) %&amp;gt;%
  tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)   0.153    0.0858       1.79 7.40e- 2
## 2 age           0.0345   0.00164     21.0  1.83e-91
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The true effect size is .0345. So we &lt;em&gt;should&lt;/em&gt; reject the null, since the coefficient is not zero (or less than zero).&lt;/p&gt;
&lt;p&gt;Now, say we have a random sample of 200 people from this population. We can take a sample, estimate 95% confidence interval below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;%
  drop_na() %&amp;gt;%
  sample_n(200) %&amp;gt;%
  specify(childs ~ age) %&amp;gt;%
  generate(reps = 1000, type = &amp;quot;bootstrap&amp;quot;) %&amp;gt;%
  calculate(stat = &amp;quot;slope&amp;quot;) %&amp;gt;%
  get_confidence_interval()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   lower_ci upper_ci
##      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1   0.0143   0.0373
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if we do this many times? How many times will our confidence interval&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;get_ci = function(x)
{
  ci = x %&amp;gt;%
  specify(childs ~ age) %&amp;gt;%
  generate(reps = 1000, type = &amp;quot;bootstrap&amp;quot;) %&amp;gt;%
  calculate(stat = &amp;quot;slope&amp;quot;) %&amp;gt;%
  get_confidence_interval()

  return(ci)
}

gss_sm %&amp;gt;%
  select(childs, age) %&amp;gt;%
  rep_sample_n(size = 200, reps = 100, replace = FALSE) %&amp;gt;%
  nest() %&amp;gt;%
  mutate(cis = map(data, get_ci))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 100 × 3
## # Groups:   replicate [100]
##    replicate data               cis             
##        &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;             &amp;lt;list&amp;gt;          
##  1         1 &amp;lt;tibble [200 × 2]&amp;gt; &amp;lt;tibble [1 × 2]&amp;gt;
##  2         2 &amp;lt;tibble [200 × 2]&amp;gt; &amp;lt;tibble [1 × 2]&amp;gt;
##  3         3 &amp;lt;tibble [200 × 2]&amp;gt; &amp;lt;tibble [1 × 2]&amp;gt;
##  4         4 &amp;lt;tibble [200 × 2]&amp;gt; &amp;lt;tibble [1 × 2]&amp;gt;
##  5         5 &amp;lt;tibble [200 × 2]&amp;gt; &amp;lt;tibble [1 × 2]&amp;gt;
##  6         6 &amp;lt;tibble [200 × 2]&amp;gt; &amp;lt;tibble [1 × 2]&amp;gt;
##  7         7 &amp;lt;tibble [200 × 2]&amp;gt; &amp;lt;tibble [1 × 2]&amp;gt;
##  8         8 &amp;lt;tibble [200 × 2]&amp;gt; &amp;lt;tibble [1 × 2]&amp;gt;
##  9         9 &amp;lt;tibble [200 × 2]&amp;gt; &amp;lt;tibble [1 × 2]&amp;gt;
## 10        10 &amp;lt;tibble [200 × 2]&amp;gt; &amp;lt;tibble [1 × 2]&amp;gt;
## # ℹ 90 more rows
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Causality</title>
      <link>/class/causality/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/causality/</guid>
      <description>&lt;h1 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h1&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;/slides/code/day1-causality.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day1-causality.R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;/slides/code/day2-causality.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day2-causality.R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Confounds</title>
      <link>/class/confounds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/confounds/</guid>
      <description>&lt;h2 id=&#34;confounding&#34;&gt;Confounding&lt;/h2&gt;
&lt;p&gt;Remember that we’re always trying to estimate the effect of some treatment, X, on some outcome, Y. The problem is that simply looking at the relationship between X and Y can lead you astray, since lots of things are correlated in the world that have no causal relationship (e.g., number of shark attacks and amount of ice cream consumed are probably positively correlated; but obviously ice cream does not cause shark attacks!).&lt;/p&gt;
&lt;p&gt;This “being led astray” by data is what we call &lt;strong&gt;confounding&lt;/strong&gt;. What we want to do in this part of the class is use causal models to figure out how to estimate the effect of X on Y, &lt;em&gt;and avoid confounding&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;marriage-divorce-age-and-confounding&#34;&gt;Marriage, divorce, age, and confounding&lt;/h2&gt;
&lt;p&gt;Remember in class that we can have a DAG like the below, where the age (A) at which people get married has a negative causal effect on the marriage (M) rate (places where people wait till later to get married have lower marriage rates) and a negative causal effect on the divorce (D) rate (places where people wait to get married have lower divorce rates).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggdag)
library(tidyverse)
library(broom)
set.seed(1990)

dagify(D ~ A, 
       M ~ A) %&amp;gt;% 
  ggdag(layout = &amp;quot;circle&amp;quot;) + 
  theme_dag()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Note that there is no arrow from marriage rate (M) to divorce (D). We have explicitly made our data so that there is no causal effect of M on D. And yet! this DAG will produce data that shows a strong relationship between M and D.&lt;/p&gt;
&lt;p&gt;Faking some data helps us see this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fake = tibble(age = rnorm(100), 
       marriage = -2*age + rnorm(100), 
       divorce = -2*age + rnorm(100))

ggplot(fake, aes(x = marriage, y = divorce)) + 
  geom_point() + geom_smooth(method = &amp;quot;lm&amp;quot;) + 
  theme_light()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We end up with a &lt;em&gt;confounded&lt;/em&gt; estimate of the effect of marriage on divorce.&lt;/p&gt;
&lt;h2 id=&#34;backdoors-and-front-doors&#34;&gt;Backdoors and front-doors&lt;/h2&gt;
&lt;p&gt;The reason that the above DAG produces a &lt;em&gt;confounded&lt;/em&gt; estimate of marriage on divorce is that there is a &lt;em&gt;backdoor path&lt;/em&gt; from marriage to divorce, going through age. A backdoor path is a path linking our X and Y variables in a &lt;em&gt;non-causal&lt;/em&gt; way. If we don’t close backdoor paths, our estimate of the effect of X (marriage) on Y (divorce) will be confounded. To close the backdoor path we must control for age.&lt;/p&gt;
&lt;p&gt;Front-door paths are the ways in which X &lt;strong&gt;causes&lt;/strong&gt; Y. They might be direct (as in an arrow from X to Y), or indirect, as in an arrow from X, to Z, to Y. We want to keep front doors open and &lt;strong&gt;not&lt;/strong&gt; accidentally control for them.&lt;/p&gt;
&lt;h2 id=&#34;the-experimental-gold-standard&#34;&gt;The experimental gold standard&lt;/h2&gt;
&lt;p&gt;Remember that in an experiment, we randomize assignment to some treatment, measure an outcome we care about, and compare whether the units that got the treatment have better or worse results on the outcome than the group that didn’t get the treatment.&lt;/p&gt;
&lt;p&gt;We can simulate a fake experiment (on rats) using R:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rats = tibble(rat_id = 1:200, 
              vaccine = sample(c(1, 0), size = 200, 
                               replace = TRUE), 
              social = sample(c(1, 0), size = 200, 
                               replace = TRUE)) %&amp;gt;% 
  mutate(covid_levels = -2*vaccine + 3*social + rnorm(200))

rats
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 200 × 4
##    rat_id vaccine social covid_levels
##     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1      1       1      0       -2.35 
##  2      2       0      1        3.74 
##  3      3       1      0        1.38 
##  4      4       1      0       -1.82 
##  5      5       1      1        1.90 
##  6      6       1      1        2.15 
##  7      7       1      0       -1.60 
##  8      8       1      1        0.923
##  9      9       1      0       -2.90 
## 10     10       0      1        4.34 
## # ℹ 190 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in making up our data, we decided that the vaccine would have an effect of -2 on covid_levels.&lt;/p&gt;
&lt;p&gt;We can estimate the effect of the vaccine on COVID using regression:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# estimate treatment effect
m1 = lm(covid_levels ~ vaccine, data = rats)
tidy(m1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)     1.56     0.181      8.64 1.89e-15
## 2 vaccine        -2.06     0.250     -8.26 2.05e-14
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the estimated coefficient on the vaccine is pretty close to the true effect (-2). The reason is because there is no &lt;em&gt;backdoor path&lt;/em&gt;, no third variable, or set of variables, causing both the vaccine &lt;strong&gt;and&lt;/strong&gt; COVID levels.&lt;/p&gt;
&lt;h2 id=&#34;easy-dags&#34;&gt;Easy DAGs&lt;/h2&gt;
&lt;p&gt;Remember, unless otherwise specified, when we are looking at DAGs we assume that &lt;em&gt;X is the treatment variable&lt;/em&gt; and &lt;em&gt;Y is the outcome variable&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag = dagify(Y ~ X)
ggdag(dag) + theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;No need to close anything here.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag = dagify(Y ~ X + B + A + C + G, 
             B ~ A + C, 
             G ~ C)
ggdag(dag) + theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;No need to close anything here either. All we care about is estimating the effect of X on Y; it doesn’t matter that other things also cause Y!&lt;/p&gt;
&lt;h2 id=&#34;when-things-go-wrong-elemental-confounds&#34;&gt;When things go wrong: Elemental confounds&lt;/h2&gt;
&lt;p&gt;We talked about three common scenarios in which confounding can take place: forks, pipes, and colliders.&lt;/p&gt;
&lt;h3 id=&#34;forks&#34;&gt;forks&lt;/h3&gt;
&lt;p&gt;Remember, the fork looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;confounder_triangle(x = &amp;quot;Has Huge beard&amp;quot;, 
                    y = &amp;quot;Amount of basketball watched&amp;quot;,
                    z = &amp;quot;Love James Harden&amp;quot;) %&amp;gt;% 
  ggdag(use_labels = &amp;quot;label&amp;quot;) +
  theme_dag()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Need to control for loving James harden!&lt;/p&gt;
&lt;p&gt;Or like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;confounder_triangle(x = &amp;quot;Marriage rate&amp;quot;, 
                    y = &amp;quot;Divorce rate&amp;quot;,
                    z = &amp;quot;Median age at marriage&amp;quot;, x_y_associated = TRUE) %&amp;gt;% 
  ggdag(use_labels = &amp;quot;label&amp;quot;) +
  theme_dag()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Need to control for median age at marriage!&lt;/p&gt;
&lt;h3 id=&#34;pipes&#34;&gt;pipes&lt;/h3&gt;
&lt;p&gt;The pipe looks like this and it’s an example of why we shouldn’t close front-door paths:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dagify(Y ~ B + G, 
       B ~ X, 
       labels = c(&amp;quot;Y&amp;quot; = &amp;quot;Plant growth&amp;quot;, 
                  &amp;quot;B&amp;quot; = &amp;quot;Good bacteria&amp;quot;, 
                  &amp;quot;G&amp;quot; = &amp;quot;Starting size&amp;quot;, 
                  &amp;quot;X&amp;quot; = &amp;quot;Fertilizer&amp;quot;)) %&amp;gt;% 
  ggdag(use_labels = &amp;quot;label&amp;quot;) + 
  theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;If we control for presence of good bacteria we are closing front-door from Fertilizer to Plant Growth (which is bad)!&lt;/p&gt;
&lt;h3 id=&#34;collider&#34;&gt;collider&lt;/h3&gt;
&lt;p&gt;The collider looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;collider_triangle(x = &amp;quot;Qualtiy research&amp;quot;, 
                  y = &amp;quot;Surprising results&amp;quot;, 
                  m = &amp;quot;Published&amp;quot;) %&amp;gt;% 
  ggdag(use_labels = &amp;quot;label&amp;quot;) + 
  theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;If we control for M here we are creating a backdoor path from X to Y. This is bad!&lt;/p&gt;
&lt;h2 id=&#34;more-complicated-dags&#34;&gt;More complicated DAGs&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag = dagify(Y ~ X + P + A + I, 
             X ~ P, 
             exposure = &amp;quot;X&amp;quot;, 
             outcome = &amp;quot;Y&amp;quot;)

ggdag(dag) + 
  theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Here, we need to control for P since it is a backdoor from X to Y. Note that there are other variables that also cause Y (I and A) but &lt;em&gt;we don’t need to control for them to estimate the effect of X on Y&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag = dagify(support ~ anxiety + event, 
             anxiety ~ event, 
             labels = c(&amp;quot;support&amp;quot; = &amp;quot;Support for torture&amp;quot;, 
                        &amp;quot;anxiety&amp;quot; = &amp;quot;Level of anxiety&amp;quot;, 
                        &amp;quot;event&amp;quot; = &amp;quot;Terrorism event&amp;quot;)
)
ggdag(dag, text = FALSE, use_labels = &amp;quot;label&amp;quot;) + theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;In this example from class, we want to know the effect of experiencing terrorism on support for torture. Notice the effect of terrorism on support for torture is both direct and indirect (via heightened anxiety). This is an example of a &lt;em&gt;pipe&lt;/em&gt;; if we control for anxiety here, we are effectively controlling away part of the effect of terrorism on support for torture.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;smoking_ca_dag = dagify(cardiacarrest ~ cholesterol,
                         cholesterol ~ smoking + weight,
                         smoking ~ unhealthy,
                         weight ~ unhealthy,
                         labels = c(&amp;quot;cardiacarrest&amp;quot; = &amp;quot;Cardiac\n Arrest&amp;quot;, 
                                    &amp;quot;smoking&amp;quot; = &amp;quot;Drugs&amp;quot;,
                                    &amp;quot;cholesterol&amp;quot; = &amp;quot;Cholesterol&amp;quot;,
                                    &amp;quot;unhealthy&amp;quot; = &amp;quot;Unhealthy\n Lifestyle&amp;quot;,
                                    &amp;quot;weight&amp;quot; = &amp;quot;Weight&amp;quot;),
                         latent = &amp;quot;unhealthy&amp;quot;,
                         exposure = &amp;quot;smoking&amp;quot;,
                         outcome = &amp;quot;cardiacarrest&amp;quot;)

ggdag(smoking_ca_dag, text = FALSE, use_labels = &amp;quot;label&amp;quot;) + theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;In this example we want to know the effect of drug use on cardiac arrest. Notice there is a backdoor path through unhealthy life styles, to weight, to cholesterol, and onto cardiac arrest. We want to close that backdoor path: we can control for unhealthy lifestyle, OR weight (notice how both break the chain!).&lt;/p&gt;
&lt;p&gt;But there is also a pipe here! If we were to accidentally control for cholesterol we would be closing a front-door path (bad). So we need to avoid doing that.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag = dagify(y ~ x + c, 
       x ~ u, 
       u ~ a, 
       c ~ a,
       b ~ u + c, 
       exposure = &amp;quot;x&amp;quot;, 
       outcome = &amp;quot;y&amp;quot;)
ggdag(dag) + theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/confounds_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Notice the backdoor path: X &amp;lt;- U -&amp;gt; A -&amp;gt; C -&amp;gt; Y. To close it and break the chain we can control for U, A, or C.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Correlation and regression</title>
      <link>/class/regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/regression/</guid>
      <description>&lt;h2 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h2&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;/slides/code/day1-models.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day1-models.R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;/slides/code/day2-models.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day2-models.R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;drawing-lines-geom_smooth&#34;&gt;Drawing lines (geom_smooth)&lt;/h2&gt;
&lt;p&gt;Why draw trend lines? Trend lines give us a good, educated guess as to what the value of a Y variable is given some value of X. We can draw a trend line (or line of best fit) using &lt;code&gt;geom_smooth&lt;/code&gt;, as below. Notice &lt;code&gt;method = &amp;quot;lm&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# libraries
library(tidyverse)

# mtcars
ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/regression_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;ggcorrplot&#34;&gt;ggcorrplot&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# libraries
library(tidyverse)
library(socviz)
library(fivethirtyeight)
library(gapminder)
library(nycflights13)
library(ggcorrplot)
library(juanr)
library(palmerpenguins)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at the correlations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# switch out gapminder with a dataset you want below
therm %&amp;gt;%
  # correlation only works with numeric columns; keep only those
  select(where(is.numeric)) %&amp;gt;%
  # the cor() function doesn&#39;t take NA; drop them all
  drop_na() %&amp;gt;%
  # get the correlation
  cor() %&amp;gt;%
  # plot the correlation
  ggcorrplot(lab = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/regression_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;draw-the-line&#34;&gt;Draw the line&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# draw a line: alter the intercept and slope in geom_abline()
# to draw the line
  ggplot() + geom_abline(intercept = 1, slope = 2, size = 1) +
  # change the limits on the x and y-axis
  scale_x_continuous(limits = c(-10, 10)) +
  scale_y_continuous(limits = c(-10, 10)) +
  # add a vertical and horizontal line at 0
  geom_hline(yintercept = 0, lty = 2) +
  geom_vline(xintercept = 0, lty = 2) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/regression_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;convince-yourself&#34;&gt;Convince yourself&lt;/h2&gt;
&lt;p&gt;Make the scatterplot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# convince yourself about the line of best fit: run this code below
# set the seed
set.seed(1990)

# make the fake data
df = tibble(x = rnorm(50, mean = 10),
            y = 3 + 2*x + rnorm(50))

# line of best fit?
model = lm(y ~ x, df)
true = tibble(true_intercept = coef(model)[1],
              true_slope = coef(model)[2])

ggplot(data = df, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/regression_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;ir-econ&#34;&gt;IR Econ&lt;/h2&gt;
&lt;p&gt;The plot&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ir_1959 = trade %&amp;gt;%
  filter(year == 2008)


ggplot(ir_1959, aes(x = imports, y = exports,
                    label = country)) + geom_point() + geom_smooth(method = &amp;quot;lm&amp;quot;) +
  geom_text()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/regression_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Estimate a model, and interpret:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(broom)

igo_pop = lm(exports ~ pop, data = trade)

tidy(igo_pop)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term            estimate     std.error statistic  p.value
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept) 13322.       1023.              13.0 1.81e-38
## 2 pop             0.000393    0.00000920      42.7 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Penguins regression:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;penguins_model = lm(body_mass_g ~ species,
                    data = penguins)

tidy(penguins_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##   term             estimate std.error statistic   p.value
##   &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)        3701.       37.6    98.4   2.49e-251
## 2 speciesChinstrap     32.4      67.5     0.480 6.31e-  1
## 3 speciesGentoo      1375.       56.1    24.5   5.42e- 77
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chinstrap penguins weigh 32 more grams, on average, than Adelie penguins.&lt;/li&gt;
&lt;li&gt;Gentoo penguins weigh 1,375 more grams, on average, than Adelie penguins.&lt;/li&gt;
&lt;li&gt;Adelie penguins weigh, on average, 3,700 grams.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;another one:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lm(tvhours ~ race, data = gss_cat) %&amp;gt;%
  tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##   term        estimate std.error statistic   p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)  2.76       0.0792    34.9   3.90e-253
## 2 raceBlack    1.42       0.100     14.1   5.90e- 45
## 3 raceWhite    0.00894    0.0838     0.107 9.15e-  1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Black respondents watch 1.42 more hours of tv, on average, than respondents who identify as “Other”.&lt;/li&gt;
&lt;li&gt;White respondents watch .009 more hours of tv, on average, than respondents who identify as “Other”.&lt;/li&gt;
&lt;li&gt;Respondents who identify as “Other” watch, on average, 2.76 hours of TV.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>DAGs</title>
      <link>/class/dags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/dags/</guid>
      <description>&lt;h1 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h1&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;/slides/code/day1-dags.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day1-dags.R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;/slides/code/day2-dags.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day2-dags.R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;making-dags-with-ggdag&#34;&gt;Making DAGs with ggdag()&lt;/h2&gt;
&lt;p&gt;To make DAGs we use &lt;code&gt;dagify()&lt;/code&gt; from the &lt;code&gt;ggdag&lt;/code&gt; package. Note the basic format:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Y ~ A + B + C + … = think of this as “Y is caused by A, B, C, …”.&lt;/li&gt;
&lt;li&gt;you need to specify the treatment variable with &lt;code&gt;exposure&lt;/code&gt; (another word for “treatment”) and the outcome variable with &lt;code&gt;outcome&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggdag)
library(tidyverse)

# make a dag
dag = dagify(Y ~ X + A + B + C, 
             X ~ A, 
             A ~ B + C, exposure = &amp;quot;X&amp;quot;, outcome = &amp;quot;Y&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then plot with &lt;code&gt;ggdag&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot it
ggdag(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dags_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can make prettier in the following ways (but no need):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggdag(dag) + theme_dag()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dags_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can use &lt;code&gt;ggdag_paths&lt;/code&gt; to identify front and backdoor paths from treatment to outcome:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggdag_paths(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dags_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can use &lt;code&gt;ggdag_adjustment_set&lt;/code&gt; to see what variables we need to control for:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggdag_adjustment_set(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dags_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can also use words instead of letters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## made up exqmple: shuttle service --&amp;gt; turnout
dag = dagify(turnout ~ shuttle + income + distance_poll + schedule_flex, 
             income ~ job + location, 
             distance_poll ~ location + car, 
             shuttle ~ cost + partisan + location, 
             exposure = &amp;quot;shuttle&amp;quot;, outcome = &amp;quot;turnout&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to use labels instead of text to make it easier to read:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggdag(dag, use_labels = &amp;quot;name&amp;quot;, text = FALSE) + theme_dag()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dags_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
</description>
    </item>
    
    <item>
      <title>Data visualization</title>
      <link>/class/dataviz/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/dataviz/</guid>
      <description>&lt;h1 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h1&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Day one: 
&lt;a href=&#34;/files/five-graphs.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;Five graphs&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Day two: 
&lt;a href=&#34;/files/viz-day2.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;Day 2&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;making-graphs-with-ggplot&#34;&gt;Making graphs with ggplot&lt;/h1&gt;
&lt;p&gt;Here we will walk through how to make some of the basic graphs in R.&lt;/p&gt;
&lt;p&gt;The code chunk below loads our libraries and prepares the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# load libraries -------------------------------------------------------------------------
library(tidyverse)
library(gapminder) # install this if you don&#39;t have it!
library(ggbeeswarm) # install this if you don&#39;t have it!
library(moderndive)

# clean data ------------------------------------------------------------------------


# subset data to focus on 2007
gap_07 =
  gapminder %&amp;gt;%
  filter(year == 2007)


# calculate average life span by year
life_yr =
  gapminder %&amp;gt;%
  select(year, lifeExp) %&amp;gt;%
  group_by(year) %&amp;gt;%
  summarise(avg_yrs = mean(lifeExp))

# calculate average life expectancy by continent
life_region =
  gap_07 %&amp;gt;%
  group_by(continent) %&amp;gt;%
  summarise(avg_yrs = mean(lifeExp))

# calculate average life expectancy by continent-year
life_region_yr =
  gapminder %&amp;gt;%
  group_by(continent, year) %&amp;gt;%
  summarise(avg_yrs = mean(lifeExp))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-scatterplot&#34;&gt;The scatterplot&lt;/h2&gt;
&lt;p&gt;The scatterplot puts two variables on the same graph so we can see how they move together:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# the fancy, final product
ggplot(gap_07, aes(x = gdpPercap, y = lifeExp,
                   color = continent, size = pop)) +
  geom_point()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can add labels using &lt;code&gt;labs()&lt;/code&gt; and a &lt;code&gt;theme&lt;/code&gt; using &lt;code&gt;theme_bw()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(gap_07, aes(x = gdpPercap, y = lifeExp,
                   color = continent, size = pop)) +
  geom_point() +
  labs(x = &amp;quot;GDP per capita ($USD, inflation-adjusted)&amp;quot;,
       y = &amp;quot;Life expectancy (in years)&amp;quot;,
       title = &amp;quot;Wealth and Health Around the World&amp;quot;,
       subtitle = &amp;quot;Data from 2007. Source: gapminder package.&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Note that there are many 
&lt;a href=&#34;https://ggplot2.tidyverse.org/reference/ggtheme.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;more themes out there&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-time-series&#34;&gt;The time series&lt;/h2&gt;
&lt;p&gt;The time series shows you how a variable moves over time, using a line. For this one we will use the &lt;code&gt;life_yr&lt;/code&gt; data object we constructed above, which looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## look at the data
life_yr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 × 2
##     year avg_yrs
##    &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1  1952    49.1
##  2  1957    51.5
##  3  1962    53.6
##  4  1967    55.7
##  5  1972    57.6
##  6  1977    59.6
##  7  1982    61.5
##  8  1987    63.2
##  9  1992    64.2
## 10  1997    65.0
## 11  2002    65.7
## 12  2007    67.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We make the plot below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# make the plot
ggplot(life_yr, aes(x = year, y = avg_yrs)) +
  geom_line() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;multiple-time-series&#34;&gt;Multiple time series&lt;/h2&gt;
&lt;p&gt;Sometimes we have data where we observe multiple &lt;em&gt;units&lt;/em&gt; moving over time, like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;life_region_yr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 60 × 3
## # Groups:   continent [5]
##    continent  year avg_yrs
##    &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 Africa     1952    39.1
##  2 Africa     1957    41.3
##  3 Africa     1962    43.3
##  4 Africa     1967    45.3
##  5 Africa     1972    47.5
##  6 Africa     1977    49.6
##  7 Africa     1982    51.6
##  8 Africa     1987    53.3
##  9 Africa     1992    53.6
## 10 Africa     1997    53.6
## # ℹ 50 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can draw separate time series for each &lt;em&gt;unit&lt;/em&gt; by either using the &lt;code&gt;color&lt;/code&gt; aesthetic to separate the lines:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(life_region_yr, aes(x = year, y = avg_yrs, color = continent)) +
  geom_line() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Or the &lt;code&gt;group&lt;/code&gt; aesthetic:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(life_region_yr, aes(x = year, y = avg_yrs, group = continent)) +
  geom_line() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;histogram-with-geom_histogram&#34;&gt;Histogram with &lt;code&gt;geom_histogram()&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(gap_07, aes(x = lifeExp)) +
  geom_histogram() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;grouped-histogram&#34;&gt;Grouped histogram&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(gap_07, aes(x = lifeExp, fill = continent)) +
  geom_histogram() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;barplot-with-geom_col&#34;&gt;Barplot with &lt;code&gt;geom_col()&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(life_region_yr, aes(x = continent, y = avg_yrs)) +
  geom_col()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;the-boxplot&#34;&gt;The boxplot&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = gapminder, aes(x = continent, y = lifeExp)) + 
  geom_boxplot()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h1 id=&#34;other-layers-and-features&#34;&gt;Other layers and features&lt;/h1&gt;
&lt;h2 id=&#34;multi-panel-plots-with-facet_wrap&#34;&gt;Multi-panel plots with &lt;code&gt;facet_wrap()&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(gapminder, aes(x = lifeExp, fill = continent)) +
  geom_histogram() +
  facet_wrap(vars(year)) +
  theme_minimal()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;make-aesthetics-static-within-the-geometries&#34;&gt;Make aesthetics static within the geometries&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) + theme_light() +
  geom_point(size = 3, color = &amp;quot;orange&amp;quot;, shape = 2, alpha = .5) #&amp;lt;&amp;lt;
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;60%&#34; /&gt;
&lt;p&gt;Take your aesthetics out of &lt;code&gt;aes()&lt;/code&gt; and into &lt;code&gt;geom()&lt;/code&gt; to make them &lt;em&gt;static&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;show-a-trend-line-using-geom_smooth&#34;&gt;Show a trend-line using &lt;code&gt;geom_smooth()&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(evals, aes(x = age, y = score)) +
  geom_point() + theme_bw() + labs(x = &amp;quot;Professor age&amp;quot;, y = &amp;quot;Student evals&amp;quot;) +
  geom_smooth() #&amp;lt;&amp;lt;
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;60%&#34; /&gt;
&lt;p&gt;Trend lines can reveal patterns in “clumpy” data&lt;/p&gt;
&lt;h2 id=&#34;show-separate-trend-lines&#34;&gt;Show &lt;em&gt;separate&lt;/em&gt; trend-lines&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(evals, aes(x = age, y = score, color = gender)) + #&amp;lt;&amp;lt;
  geom_point() + theme_bw() + labs(x = &amp;quot;Professor age&amp;quot;, y = &amp;quot;Student evals&amp;quot;) +
  geom_smooth() #&amp;lt;&amp;lt;
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;60%&#34; /&gt;
&lt;p&gt;Relationships can look different &lt;em&gt;within&lt;/em&gt; groups&lt;/p&gt;
&lt;h2 id=&#34;use-different-color-and-fill-scales&#34;&gt;Use different color and fill scales&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(gapminder, aes(x = lifeExp, fill = continent)) +
  geom_histogram() +
  scale_fill_brewer(palette = &amp;quot;Blues&amp;quot;) #&amp;lt;&amp;lt;
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;60%&#34; /&gt;
&lt;p&gt;&lt;code&gt;fill_brewer()&lt;/code&gt; for &lt;code&gt;fill&lt;/code&gt;, &lt;code&gt;color_brewer&lt;/code&gt; for &lt;code&gt;color&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;many-other-themes&#34;&gt;Many other themes&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tvthemes)
ggplot(gapminder, aes(x = lifeExp, fill = continent)) +
  geom_histogram() + theme_spongeBob() + labs(title = &amp;quot;Horrible&amp;quot;) +
  scale_fill_spongeBob()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/dataviz_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;80%&#34; /&gt;
&lt;p&gt;&lt;code&gt;theme_spongeBob()&lt;/code&gt; from &lt;code&gt;tvthemes&lt;/code&gt; package, many more online&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>/class/data-wrangle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/data-wrangle/</guid>
      <description>&lt;h2 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h2&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/slides/code/wrangle-day1.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;wrangle-day1.R&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;filtering&#34;&gt;Filtering&lt;/h2&gt;
&lt;p&gt;Often, we have a big dataset that covers lots of stuff (say, all flights coming out of NYC in 2013) but we’re only interested in a &lt;em&gt;subset&lt;/em&gt; of those things (say, flights that arrived late over that time period). The &lt;code&gt;filter()&lt;/code&gt; function is a way to subset operations that match some rule or set of rules (e.g., rule = “flights that arrived late”). We define these rules using 
&lt;a href=&#34;https://www.statmethods.net/management/operators.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;logical operators&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;p&gt;Let’s load the libraries.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# libraries
library(tidyverse)
library(nycflights13)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember you can look at the data like this.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# look at the data
View(flights) # open data in viewer
?flights # read data documentation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at flights from February.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# look at fights, but only from February
flights %&amp;gt;%
  filter(month == 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 24,951 × 19
##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;
##  1  2013     2     1      456            500        -4      652            648
##  2  2013     2     1      520            525        -5      816            820
##  3  2013     2     1      527            530        -3      837            829
##  4  2013     2     1      532            540        -8     1007           1017
##  5  2013     2     1      540            540         0      859            850
##  6  2013     2     1      552            600        -8      714            715
##  7  2013     2     1      552            600        -8      919            910
##  8  2013     2     1      552            600        -8      655            709
##  9  2013     2     1      553            600        -7      833            815
## 10  2013     2     1      553            600        -7      821            825
## # ℹ 24,941 more rows
## # ℹ 11 more variables: arr_delay &amp;lt;dbl&amp;gt;, carrier &amp;lt;chr&amp;gt;, flight &amp;lt;int&amp;gt;,
## #   tailnum &amp;lt;chr&amp;gt;, origin &amp;lt;chr&amp;gt;, dest &amp;lt;chr&amp;gt;, air_time &amp;lt;dbl&amp;gt;, distance &amp;lt;dbl&amp;gt;,
## #   hour &amp;lt;dbl&amp;gt;, minute &amp;lt;dbl&amp;gt;, time_hour &amp;lt;dttm&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at flights on Valentine’s Day.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# now let&#39;s look at flights on Valentine&#39;s Day
flights %&amp;gt;%
  filter(month == 2) %&amp;gt;%
  filter(day == 14)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 956 × 19
##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;
##  1  2013     2    14        7           2352        15      448            437
##  2  2013     2    14       59           2339        80      205            106
##  3  2013     2    14      454            500        -6      641            648
##  4  2013     2    14      510            515        -5      750            814
##  5  2013     2    14      531            530         1      828            831
##  6  2013     2    14      541            540         1      850            850
##  7  2013     2    14      542            545        -3     1014           1023
##  8  2013     2    14      551            600        -9      831            906
##  9  2013     2    14      552            600        -8      657            708
## 10  2013     2    14      553            600        -7      902            856
## # ℹ 946 more rows
## # ℹ 11 more variables: arr_delay &amp;lt;dbl&amp;gt;, carrier &amp;lt;chr&amp;gt;, flight &amp;lt;int&amp;gt;,
## #   tailnum &amp;lt;chr&amp;gt;, origin &amp;lt;chr&amp;gt;, dest &amp;lt;chr&amp;gt;, air_time &amp;lt;dbl&amp;gt;, distance &amp;lt;dbl&amp;gt;,
## #   hour &amp;lt;dbl&amp;gt;, minute &amp;lt;dbl&amp;gt;, time_hour &amp;lt;dttm&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s try the OR logical operator by looking at flights going to ATL or SFO.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# try one using text and the OR symbol
# look at fights going to ATL or SFO
flights %&amp;gt;%
  filter(dest == &amp;quot;ATL&amp;quot; | dest == &amp;quot;SFO&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30,546 × 19
##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;
##  1  2013     1     1      554            600        -6      812            837
##  2  2013     1     1      558            600        -2      923            937
##  3  2013     1     1      600            600         0      837            825
##  4  2013     1     1      606            610        -4      837            845
##  5  2013     1     1      611            600        11      945            931
##  6  2013     1     1      615            615         0      833            842
##  7  2013     1     1      655            700        -5     1037           1045
##  8  2013     1     1      658            700        -2      944            939
##  9  2013     1     1      729            730        -1     1049           1115
## 10  2013     1     1      734            737        -3     1047           1113
## # ℹ 30,536 more rows
## # ℹ 11 more variables: arr_delay &amp;lt;dbl&amp;gt;, carrier &amp;lt;chr&amp;gt;, flight &amp;lt;int&amp;gt;,
## #   tailnum &amp;lt;chr&amp;gt;, origin &amp;lt;chr&amp;gt;, dest &amp;lt;chr&amp;gt;, air_time &amp;lt;dbl&amp;gt;, distance &amp;lt;dbl&amp;gt;,
## #   hour &amp;lt;dbl&amp;gt;, minute &amp;lt;dbl&amp;gt;, time_hour &amp;lt;dttm&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at flights between noon and 5pm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# try one using greater than or less than
# look at flights departing between 12pm and 5pm
flights %&amp;gt;%
  filter(dep_time &amp;gt;= 1200) %&amp;gt;%
  filter(dep_time &amp;lt;= 1700)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 99,136 × 19
##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt;
##  1  2013     1     1     1200           1200         0     1408           1356
##  2  2013     1     1     1202           1207        -5     1318           1314
##  3  2013     1     1     1202           1159         3     1645           1653
##  4  2013     1     1     1203           1205        -2     1501           1437
##  5  2013     1     1     1203           1200         3     1519           1545
##  6  2013     1     1     1204           1200         4     1500           1448
##  7  2013     1     1     1205           1200         5     1503           1505
##  8  2013     1     1     1206           1209        -3     1325           1328
##  9  2013     1     1     1208           1158        10     1540           1502
## 10  2013     1     1     1211           1215        -4     1423           1413
## # ℹ 99,126 more rows
## # ℹ 11 more variables: arr_delay &amp;lt;dbl&amp;gt;, carrier &amp;lt;chr&amp;gt;, flight &amp;lt;int&amp;gt;,
## #   tailnum &amp;lt;chr&amp;gt;, origin &amp;lt;chr&amp;gt;, dest &amp;lt;chr&amp;gt;, air_time &amp;lt;dbl&amp;gt;, distance &amp;lt;dbl&amp;gt;,
## #   hour &amp;lt;dbl&amp;gt;, minute &amp;lt;dbl&amp;gt;, time_hour &amp;lt;dttm&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at how many flights arrived late on christmas day.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## how many flights arrived LATE, on christmas day?
late_xmas = flights %&amp;gt;%
  filter(arr_time &amp;gt; sched_arr_time) %&amp;gt;%
  filter(month == 12, day == 25)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;leaders&#34;&gt;Leaders&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(juanr)
leader
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17,686 × 16
##    country gwcode leader gender  year yr_office   age edu     mil_service combat
##    &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;         &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 USA          2 Grant  M       1869         1    47 Univer…           1      1
##  2 USA          2 Grant  M       1870         2    48 Univer…           1      1
##  3 USA          2 Grant  M       1871         3    49 Univer…           1      1
##  4 USA          2 Grant  M       1872         4    50 Univer…           1      1
##  5 USA          2 Grant  M       1873         5    51 Univer…           1      1
##  6 USA          2 Grant  M       1874         6    52 Univer…           1      1
##  7 USA          2 Grant  M       1875         7    53 Univer…           1      1
##  8 USA          2 Grant  M       1876         8    54 Univer…           1      1
##  9 USA          2 Grant  M       1877         9    55 Univer…           1      1
## 10 USA          2 Hayes  M       1877         1    55 Gradua…           1      1
## # ℹ 17,676 more rows
## # ℹ 6 more variables: rebel &amp;lt;dbl&amp;gt;, yrs_exp &amp;lt;dbl&amp;gt;, phys_health &amp;lt;dbl&amp;gt;,
## #   mental_health &amp;lt;dbl&amp;gt;, will_force &amp;lt;dbl&amp;gt;, will_force_sd &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;A Vietnamese Emperor who, in his first year in office, was 11 years old. Famously depraved.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;leader %&amp;gt;%
  # first year in office
  filter(yr_office == 1) %&amp;gt;%
  # age at that point
  filter(age == 11) %&amp;gt;%
  # vietnamese
  filter(country == &amp;quot;VNM&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 16
##   country gwcode leader    gender  year yr_office   age edu   mil_service combat
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 VNM        815 Thanh Th… M       1889         1    11 Seco…           0      0
## # ℹ 6 more variables: rebel &amp;lt;dbl&amp;gt;, yrs_exp &amp;lt;dbl&amp;gt;, phys_health &amp;lt;dbl&amp;gt;,
## #   mental_health &amp;lt;dbl&amp;gt;, will_force &amp;lt;dbl&amp;gt;, will_force_sd &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Leaders with graduate degrees who in 2015 reached their 16th year in power.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;leader %&amp;gt;%
  filter(edu == &amp;quot;Graduate&amp;quot;, yr_office == 16, year == 2015)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 16
##   country gwcode leader    gender  year yr_office   age edu   mil_service combat
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 RUS        365 Putin     M       2015        16    63 Grad…           0      0
## 2 SYR        652 Bashar a… M       2015        16    50 Grad…           1      0
## # ℹ 6 more variables: rebel &amp;lt;dbl&amp;gt;, yrs_exp &amp;lt;dbl&amp;gt;, phys_health &amp;lt;dbl&amp;gt;,
## #   mental_health &amp;lt;dbl&amp;gt;, will_force &amp;lt;dbl&amp;gt;, will_force_sd &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;The number of world leaders in the post-2000 period who have known physical or mental health issues.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;leader %&amp;gt;%
  filter((year &amp;gt; 2000) &amp;amp; (phys_health == 1 | mental_health == 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 103 × 16
##    country gwcode leader   gender  year yr_office   age edu   mil_service combat
##    &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 CAN         20 Chretien M       2001         9    62 Univ…           0      0
##  2 CAN         20 Chretien M       2002        10    63 Univ…           0      0
##  3 CAN         20 Chretien M       2003        11    64 Univ…           0      0
##  4 SLV        349 Drnovsek M       2001         2    51 Grad…           0      0
##  5 SLV        349 Drnovsek M       2002         3    52 Grad…           0      0
##  6 BLR        370 Lukashe… M       2001         8    47 Grad…           1      0
##  7 BLR        370 Lukashe… M       2002         9    48 Grad…           1      0
##  8 BLR        370 Lukashe… M       2003        10    49 Grad…           1      0
##  9 BLR        370 Lukashe… M       2004        11    50 Grad…           1      0
## 10 BLR        370 Lukashe… M       2005        12    51 Grad…           1      0
## # ℹ 93 more rows
## # ℹ 6 more variables: rebel &amp;lt;dbl&amp;gt;, yrs_exp &amp;lt;dbl&amp;gt;, phys_health &amp;lt;dbl&amp;gt;,
## #   mental_health &amp;lt;dbl&amp;gt;, will_force &amp;lt;dbl&amp;gt;, will_force_sd &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;mutating&#34;&gt;Mutating&lt;/h2&gt;
&lt;p&gt;Sometimes we want to create new variables. For example, we might want to combine or alter existing variables in our dataset. The &lt;code&gt;mutate()&lt;/code&gt; function is one way of doing this.&lt;/p&gt;
&lt;p&gt;Let’s convert arrival delay from minutes to hours.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## convert arrival_delay to hours
new_flights = flights %&amp;gt;%
  mutate(arr_delay_hrs = arr_delay/60)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look in the dataset you will see a new variable called &lt;code&gt;arr_delay_hrs&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s convert distance traveled from miles to thousands of miles.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## convert distance to thousands of miles
new_flights2 = flights %&amp;gt;%
  mutate(dist_miles = distance/1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;creating-categorical-variables&#34;&gt;Creating categorical variables&lt;/h3&gt;
&lt;p&gt;Sometimes we want to create more complicated variables. Here’s where 
&lt;a href=&#34;https://dplyr.tidyverse.org/reference/case_when.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;case_when&lt;/a&gt; comes into play.&lt;/p&gt;
&lt;p&gt;Let’s create a variable that tells us what season a flight took off in.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## create a new variable called season
## that tells me if flight departed
## in summer, winter, fall, or spring
new_flights = flights %&amp;gt;%
  mutate(seasons = case_when(month == 6 ~ &amp;quot;Summer&amp;quot;,
                             month == 7 ~ &amp;quot;Summer&amp;quot;,
                             month == 8 ~ &amp;quot;Summer&amp;quot;,
                             month == 9 ~ &amp;quot;Fall&amp;quot;,
                             month == 10 ~ &amp;quot;Fall&amp;quot;,
                             month == 11 ~ &amp;quot;Fall&amp;quot;,
                             month == 12 ~ &amp;quot;Winter&amp;quot;,
                             month == 1 ~ &amp;quot;Winter&amp;quot;,
                             month == 2 ~ &amp;quot;Winter&amp;quot;,
                             month == 3 ~ &amp;quot;Spring&amp;quot;,
                             month == 4 ~ &amp;quot;Spring&amp;quot;,
                             month == 5 ~ &amp;quot;Spring&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then plot the distribution of arrival delays by season, below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot histogram of arrival delay
# separate it by season
ggplot(new_flights, aes(x = arr_delay, fill = seasons)) +
  geom_histogram() +
  facet_wrap(vars(seasons))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

## Warning: Removed 9430 rows containing non-finite values (`stat_bin()`).
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-wrangle_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Let’s say we wanted to categorize flights by how late they are. See an example, below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;new_flights = flights %&amp;gt;%
  mutate(time_flight = case_when(arr_delay &amp;gt;= 120 ~ &amp;quot;very late&amp;quot;,
                                 arr_delay &amp;gt; 0 &amp;amp; arr_delay &amp;lt; 120 ~ &amp;quot;a little late&amp;quot;,
                                 arr_delay == 0 ~ &amp;quot;on time&amp;quot;,
                                 arr_delay &amp;lt; 0 &amp;amp; arr_delay &amp;gt; -120 ~ &amp;quot;a little early&amp;quot;,
                                 arr_delay &amp;lt;=-120 ~ &amp;quot;very early&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Fixed Effects &#43; DID</title>
      <link>/class/fixed-effects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/fixed-effects/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggdag)
library(tidyverse)
library(broom)
library(fixest)
library(gapminder)

# set seed
set.seed(1990)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-fixed-effects-dag&#34;&gt;The fixed effects DAG&lt;/h2&gt;
&lt;p&gt;Say we wanted to estimate the effect of GDP on life expectancy, and had a DAG like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dagify(Life ~ GDP + Geography + Population + Pollution + WW2 + Equator, 
       GDP ~ Geography + Population + Pollution + WW2 + Equator, 
       exposure = &amp;quot;GDP&amp;quot;, outcome = &amp;quot;Life&amp;quot;) %&amp;gt;% 
  ggdag_status(text = FALSE, use_labels = &amp;quot;name&amp;quot;) + theme_dag(legend.position = &amp;quot;none&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/fixed-effects_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;The key thing here is that some of these variables only vary &lt;em&gt;across&lt;/em&gt; countries, but not within them. They are &lt;em&gt;constant within the country&lt;/em&gt;. For example, a country’s distance from the equator is fixed. Same with whether or not they participated in WW2. Other variables &lt;em&gt;also vary within country&lt;/em&gt;, like a country’s level of pollution (or population) which changes over time.&lt;/p&gt;
&lt;p&gt;The insight is that we can think about all of the variables that are &lt;em&gt;constant within country&lt;/em&gt; as a general “country” backdoor. The “country” backdoor is all of the differences between the countries in our data that are static or fixed. Pollution and population are not included because they also vary within country.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dagify(Life ~ GDP + Country + Population + Pollution, 
       GDP ~ Country + Population + Pollution, 
       exposure = &amp;quot;GDP&amp;quot;, outcome = &amp;quot;Life&amp;quot;) %&amp;gt;% 
  ggdag_status(text = FALSE, use_labels = &amp;quot;name&amp;quot;) + theme_dag(legend.position = &amp;quot;none&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/fixed-effects_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can use fixed effects to control for the “country” backdoor, and implicitly, all variables that are static within countries.&lt;/p&gt;
&lt;p&gt;Here’s the naive regression, without country fixed effects:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# naive
m1 = lm(lifeExp ~ gdpPercap + pop, data = gapminder) 
tidy(m1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##   term        estimate     std.error statistic   p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)  5.36e+1 0.322            166.   0        
## 2 gdpPercap    7.68e-4 0.0000257         29.9  4.04e-158
## 3 pop          9.73e-9 0.00000000238      4.08 4.72e-  5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the fixed effect regression, using the &lt;code&gt;fixest&lt;/code&gt; package. The general template is: &lt;code&gt;feols(Y ~ X1 + X2 + ... | UNIT, data = DATA)&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# naive
m2 = feols(lifeExp ~ gdpPercap + pop | country, data = gapminder) 
tidy(m2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term          estimate    std.error statistic  p.value
##   &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 gdpPercap 0.000394     0.000177          2.22 0.0279  
## 2 pop       0.0000000620 0.0000000180      3.44 0.000776
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how the estimate on GDP changes with the addition of FE. Notice also that we still haven’t closed all the backdoors in our original DAG! Pollution is a backdoor, and varies within country, so the country fixed effect will not suffice. We need to control for it, but we have no data on it! This captures the broad takeaway of FE:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;FE help us close backdoors that are constant within the unit&lt;/li&gt;
&lt;li&gt;But we still need to control for confounds that vary within the unit (e.g., population, pollution)&lt;/li&gt;
&lt;li&gt;We can add those alongside FE (e.g., population)&lt;/li&gt;
&lt;li&gt;but sometimes we don’t have data on all confounds that vary within unit, so we’re not out of the woods (e.g., pollution)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;simulation-to-show-how-it-works&#34;&gt;Simulation to show how it works&lt;/h2&gt;
&lt;p&gt;We’re gonna make up some data to estimate effect of a teacher having an MA on their student’s test scores. The DAG looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# draw the dag
dag = dagify(score ~ teacher_ma + male + parent_income,
       teacher_ma ~ male + parent_income, 
       exposure = &amp;quot;teacher_ma&amp;quot;, outcome = &amp;quot;score&amp;quot;)

ggdag_status(dag, text = FALSE, use_labels = &amp;quot;name&amp;quot;) + theme_dag()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/fixed-effects_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Notice we have some backdoors that we need to control for:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggdag_adjustment_set(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/fixed-effects_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Let’s make up the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# make up student data
kids = tibble(student = c(&amp;quot;Joe&amp;quot;, &amp;quot;Jessica&amp;quot;, &amp;quot;Laia&amp;quot;, &amp;quot;Jeff&amp;quot;, &amp;quot;Martin&amp;quot;), 
       male = sample(c(1, 0), size = 5, replace = TRUE), 
       parent_income = rnorm(5))


fake = crossing(student = unique(kids$student), 
         test = 1:50) %&amp;gt;% 
  left_join(kids) %&amp;gt;% 
  mutate(teacher_ma = 2*male + 3*parent_income + rnorm(250)) %&amp;gt;% 
  mutate(score = 5*teacher_ma + -2*male + 5*parent_income + rnorm(250))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice above that the &lt;em&gt;true effect&lt;/em&gt; of &lt;code&gt;teacher_ma&lt;/code&gt; on &lt;code&gt;score&lt;/code&gt; is 5.&lt;/p&gt;
&lt;p&gt;Here’s the (wrong) result when we don’t control for anything:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# naive regression
lm(score ~ teacher_ma, data = fake) %&amp;gt;% tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)    -2.30    0.144      -16.0 5.30e- 40
## 2 teacher_ma      5.22    0.0639      81.6 3.12e-181
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the (right) result when we control for all confounds:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# correctly specify controls
lm(score ~ teacher_ma + parent_income + male, data = fake) %&amp;gt;% tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 5
##   term          estimate std.error statistic   p.value
##   &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)      0.264    0.154       1.72 8.72e-  2
## 2 teacher_ma       5.03     0.0628     80.1  3.04e-178
## 3 parent_income    4.79     0.310      15.4  4.98e- 38
## 4 male            -2.39     0.210     -11.4  1.74e- 24
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the (right) result when we use student FE:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# use fixed effects to account for student-constant variables
feols(score ~ teacher_ma | student, data = fake) %&amp;gt;% tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 5
##   term       estimate std.error statistic      p.value
##   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 teacher_ma     5.04    0.0444      113. 0.0000000362
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice above how we get the right answer &lt;em&gt;even without&lt;/em&gt; controlling for the &lt;code&gt;parent_income&lt;/code&gt; and &lt;code&gt;male&lt;/code&gt; backdoors. This is because we are implicitly controlling for them.&lt;/p&gt;
&lt;h2 id=&#34;diff-in-diff-by-hand&#34;&gt;Diff-in-diff by hand&lt;/h2&gt;
&lt;p&gt;Remember in class we were looking at the effect of Pokemon Go on exercise using difference-in-differences. Let’s see how this works by making up some data where we already know the effect of the app on exercise (let’s set the effect to 2).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(1990)
#Create our data
fake_pokemon = 
  tibble(year = sample(2002:2010,10000,replace=T), 
         app = sample(c(&amp;quot;Has app&amp;quot;, &amp;quot;Doesn&#39;t have app&amp;quot;), 10000, replace = TRUE)) %&amp;gt;% 
  mutate(after = ifelse(year &amp;gt; 2007, 1, 0)) %&amp;gt;% 
  mutate(D = after*(app == &amp;quot;Has app&amp;quot;)) %&amp;gt;% 
  mutate(Y = 2*D + .25*year + (app == &#39;Has app&#39;) +  rnorm(10000))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can find that exact difference by filling out the 2x2 before/after treatment/control table:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Before 2016&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;After 2016&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;∆&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Doesn’t have app&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;A&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;B&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;B − A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has app&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;C&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;D − C&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;∆&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;C − A&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;D − B&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;(D − C) − (B − A)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here’s the table in our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Now, get before-after differences for both groups
means = fake_pokemon %&amp;gt;% group_by(app,after) %&amp;gt;% summarize(Y=mean(Y))
means %&amp;gt;% pivot_wider(names_from = &amp;quot;after&amp;quot;, values_from = &amp;quot;Y&amp;quot;) %&amp;gt;% 
  rename(After = `1`, Before = `0`)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 3
## # Groups:   app [2]
##   app              Before After
##   &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Doesn&#39;t have app   501.  502.
## 2 Has app            502.  505.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s calculate all of the differences we need. Here’s B - A:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;502 - 501
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the change in exercise in the pre to post-period among those who didn’t receive treatment (i.e., download the app). The exercise among these kids increased by 1. Why? Remember, all sorts of thing are happening in the world at any given time. And the app was released in summer! So this increase might just be because kids are out of school, or its nicer out, etc.&lt;/p&gt;
&lt;p&gt;Let’s calculate D - C:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;505 - 502
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the change in exercise in the pre to post-period among those who did receive treatment (i.e., downloaded the app). The exercise among these kids increased by 3. Is this increase causal? No! Notice above that exercise also went up a bit among students who &lt;em&gt;didn’t&lt;/em&gt; even have the app. So exercise went up over this time (because school’s out, it’s nice out, etc.). We need to &lt;em&gt;remove&lt;/em&gt; this general increase in exercise.
Let’s get the diff-in-diff estimate:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;(505 - 502) - (502 - 501)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The diff-in-diff estimate is that exercise went up by 2 as a result of Pokemon Go.&lt;/p&gt;
&lt;h2 id=&#34;diff-in-diff-with-regression&#34;&gt;Diff-in-diff with regression&lt;/h2&gt;
&lt;p&gt;We can also do diff-in-diff via regression (in fact this is how everyone does it). The basic template is: &lt;code&gt;lm(Y ~ TREATMENT + TIME + TREATMENT*TIME, data = DATA)&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt; is our outcome of interest (here: exercise)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TREATMENT&lt;/code&gt; is a variable that tells us an observation’s treatment status (here: has pokemon go vs. doesn’t have it)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TIME&lt;/code&gt; is a variable that tells us if an observation is pre- or post-treatment (here: has pokemon go been released yet?)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TREATMENT*TIME&lt;/code&gt; is the &lt;strong&gt;interaction&lt;/strong&gt; of these two variables (more on this later)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m1 = lm(Y ~ app + after + app*after, data = fake_pokemon)
tidy(m1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 5
##   term             estimate std.error statistic   p.value
##   &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)        501.      0.0184   27233.  0        
## 2 appHas app           1.01    0.0259      39.0 1.44e-309
## 3 after                1.13    0.0321      35.2 8.69e-256
## 4 appHas app:after     1.99    0.0452      43.9 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficient for the interaction term (&lt;code&gt;appHas app:after&lt;/code&gt;) is our diff-in-diff estimate. Here we would say that Pokemon Go increased the exercise rate by 2 among app users.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Natural Experiments</title>
      <link>/class/controls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/controls/</guid>
      <description>&lt;h1 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h1&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/slides/code/socviz-maps.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;socviz-maps.R&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;closing-backdoors-with-controls&#34;&gt;Closing backdoors with controls&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggdag)
library(tidyverse)
library(broom)

# set seed
set.seed(1990)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that one of the lessons from our week on DAGs is that in order to estimate the effect of X on Y, sometimes we need to control for some variables and &lt;em&gt;avoid&lt;/em&gt; controlling for others.&lt;/p&gt;
&lt;h3 id=&#34;dealing-with-forks&#34;&gt;Dealing with forks&lt;/h3&gt;
&lt;p&gt;Here’s an example dealing with forks. The classic fork looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag = dagify(Y ~ Z, 
       X ~ Z, 
       exposure = &amp;quot;X&amp;quot;, 
       outcome = &amp;quot;Y&amp;quot;)
ggdag(dag) + theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/controls_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Notice that we need to control for Z:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggdag_adjustment_set(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/controls_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Let’s simulate some fake data from this DAG. Notice that X does not cause Y:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# fake data
fake = tibble(person = 1:200, 
       z = rnorm(200), 
       x = rnorm(200) + 2*z, 
       y = rnorm(200) - 3*z)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And yet when we look at X and Y there’s a relationship!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(fake, aes(x = x, y = y)) + 
  geom_point() + geom_smooth(method = &amp;quot;lm&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/controls_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can see this relationship in regression too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# naive regression
lm(y ~ x, data = fake) %&amp;gt;% tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)   0.0655    0.125      0.523 6.01e- 1
## 2 x            -1.20      0.0524   -23.0   1.07e-57
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But notice that when we correcty control for Z, the estimate on X goes to 0:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# with controls
lm(y ~ x + z, data = fake) %&amp;gt;% tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)   0.0515    0.0741     0.694 4.88e- 1
## 2 x            -0.0245    0.0689    -0.356 7.22e- 1
## 3 z            -2.99      0.156    -19.1   7.53e-47
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dealing-with-pipes&#34;&gt;Dealing with pipes&lt;/h3&gt;
&lt;p&gt;Here’s an example dealing with pipes The classic pipe looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag = dagify(Y ~ Z, 
             Z ~ X, 
             exposure = &amp;quot;X&amp;quot;, 
             outcome = &amp;quot;Y&amp;quot;)
ggdag(dag) + theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/controls_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Notice that we &lt;strong&gt;shouldn’t&lt;/strong&gt; control for Z:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggdag_adjustment_set(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/controls_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Let’s simulate some fake data from this DAG. Notice that X -&amp;gt; Y -&amp;gt; Z:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# fake data
fake = tibble(person = 1:200, 
              x = rnorm(200), 
              z = rnorm(200) + 2*x, 
              y = rnorm(200) - 3*z)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we look at X and Y, there’s a relationship:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(fake, aes(x = x, y = y)) + 
  geom_point() + geom_smooth(method = &amp;quot;lm&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/controls_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can see this relationship in regression too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lm(y ~ x, data = fake) %&amp;gt;% tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)    0.302     0.217      1.39 1.65e- 1
## 2 x             -6.17      0.235    -26.3  1.81e-66
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But notice that when we &lt;strong&gt;incorrectly&lt;/strong&gt; control for Z, the estimate on X goes to 0:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# with controls
lm(y ~ x + z, data = fake) %&amp;gt;% tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)  0.00961    0.0698     0.138 8.91e- 1
## 2 x           -0.0667     0.164     -0.406 6.85e- 1
## 3 z           -2.99       0.0718   -41.7   1.03e-99
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dealing-with-colliders&#34;&gt;Dealing with colliders&lt;/h3&gt;
&lt;p&gt;Here’s an example dealing with colliders The classic collider looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dag = dagify(Z ~ Y + X, 
             exposure = &amp;quot;X&amp;quot;, 
             outcome = &amp;quot;Y&amp;quot;)
ggdag(dag) + theme_dag_blank()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/controls_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Notice that we &lt;strong&gt;shouldn’t&lt;/strong&gt; control for Z:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggdag_adjustment_set(dag)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/controls_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Let’s simulate some fake data from this DAG. Notice that X does not cause Y:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# fake data
fake = tibble(person = 1:200, 
              x = rnorm(200), 
              y = rnorm(200),
              z = rnorm(200) + 2*x + -3*y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we look at X and Y alone, there’s no relationship:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(fake, aes(x = x, y = y)) + 
  geom_point() + geom_smooth(method = &amp;quot;lm&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/controls_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;We can see this relationship in regression too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# naive regression
lm(y ~ x, data = fake) %&amp;gt;% tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)  -0.0602    0.0765    -0.788   0.432
## 2 x            -0.0991    0.0738    -1.34    0.181
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But notice that when we &lt;strong&gt;incorrectly&lt;/strong&gt; control for Z, the estimate on X goes away from 0:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# with controls
lm(y ~ x + z, data = fake) %&amp;gt;% tidy()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##   term        estimate std.error statistic   p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)  0.00726   0.0235      0.309 7.58e-  1
## 2 x            0.577     0.0274     21.1   2.62e- 52
## 3 z           -0.305     0.00698   -43.7   2.68e-103
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Prediction</title>
      <link>/class/prediction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/prediction/</guid>
      <description>&lt;h2 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h2&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;/slides/code/day1-prediction.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day1-prediction.R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;predictions-simple-model-one-explanatory-variable&#34;&gt;Predictions (simple model, one explanatory variable)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# libraries
library(tidyverse)
library(broom)
library(moderndive)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s say we want to use mtcars to make predictions about a car’s fuel efficency (mpg) using a car’s weight (wt).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# data
head(mtcars)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we fit a model and look at the output.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# fitting a model of mpg ~ weight
mod = lm(mpg ~ wt, data = mtcars)
tidy(mod)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)    37.3      1.88      19.9  8.24e-19
## 2 wt             -5.34     0.559     -9.56 1.29e-10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we define a scenario we want a prediction for. For example, what MPG should we expect with a car that weighs 1.05 tons?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;scenario = crossing(wt = 1.05)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we use the &lt;code&gt;augment&lt;/code&gt; function to get our prediction. We give it our model and our scenario.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# get my prediction with augment()
augment(mod, newdata = scenario)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##      wt .fitted
##   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1  1.05    31.7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also do this by hand (plus or minus some rounding!). We just take our regression equation and plug in 1.05 for weight:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;37.3 + -5.34*1.05
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31.693
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can make more complicated predictions. For example, predicted MPG for cars with weights 1 through 5:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# what is the predicted mpg, for a car with: 
# wt = 1, 2, 3, 4, 5 tons?
scenario = crossing(wt = c(1, 2, 3, 4, 5))

# get prediction (i saved as data object to plot below)
preds_wt = augment(mod, newdata = scenario)

# look at predictions
preds_wt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 2
##      wt .fitted
##   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     1    31.9
## 2     2    26.6
## 3     3    21.3
## 4     4    15.9
## 5     5    10.6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could use this to plot our predictions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(preds_wt, aes(x = wt, y = .fitted)) + 
  # the stuff in geom_col is just for aesthetic purposes!
  geom_col(color = &amp;quot;white&amp;quot;, fill = &amp;quot;red&amp;quot;, alpha = .8) + 
  theme_light() + 
  labs(x = &amp;quot;Weight (in tons)&amp;quot;, y = &amp;quot;Predicted fuel efficiency&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/prediction_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;more-complicated-models-multiple-explanatory-variables&#34;&gt;More complicated models (multiple explanatory variables)&lt;/h2&gt;
&lt;p&gt;The real power of modeling is in using more than one explanatory variable. Below, we can model MPG using car weight (wt), number of cylinders (cyl), horsepower (hp), and the shape of the engine (vs; vs = 0 is v-shaped, vs = 1 is straight).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mod = lm(mpg ~ wt + cyl + hp + vs, data = mtcars)
tidy(mod)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)  38.5       3.34     11.5    6.14e-12
## 2 wt           -3.18      0.774    -4.11   3.26e- 4
## 3 cyl          -0.905     0.679    -1.33   1.94e- 1
## 4 hp           -0.0179    0.0122   -1.46   1.56e- 1
## 5 vs            0.155     1.62      0.0957 9.24e- 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then make use of these explanatory variables to make (hopefully) more precise predictions about the outcome we care about. Here’s an example predicting fuel efficiency for a 1 ton car, with a 4 cylinder engine, with 120 horsepower, and a straight engine:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# set scenario
scen = crossing(wt = 3, 
              cyl = 4, 
              hp = 160, 
              vs = 0)

# get prediction
augment(mod, newdata = scen)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 5
##      wt   cyl    hp    vs .fitted
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     3     4   160     0    22.5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can also get predictions where I vary the values one variable takes on and leave the others at a constant value:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# i have a car that weighs 1 ton, hp = 100, vs = 0, 
# what does fuel efficiency look like, if I have 4, 6, 8?
scenario = crossing(wt = 1, 
                  hp = 100, 
                  vs = 0, 
                  cyl = c(4, 6, 8))

augment(mod, newdata = scenario)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##      wt    hp    vs   cyl .fitted
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     1   100     0     4    29.9
## 2     1   100     0     6    28.1
## 3     1   100     0     8    26.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;interpreting-regression-output&#34;&gt;Interpreting regression output&lt;/h2&gt;
&lt;p&gt;Remember, the basic template for interpreting regression coefficients for &lt;strong&gt;continuous variables&lt;/strong&gt; is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For every unit increase in EXPLANATORY VARIABLE there is an associated COEFFICIENT ESTIMATE change in OUTCOME VARIABLE&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The basic template for interpreting regression coefficients for &lt;strong&gt;categorical variables&lt;/strong&gt; is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;observations with CATEGORY have, on average, COEFFICIENT ESTIMATE more/less OUTCOME VARIABLE than observations with BASELINE CATEGORY&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And the template for interpreting the &lt;em&gt;intercept&lt;/em&gt; is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The average value of the OUTCOME VARIABLE when all EXPLANATORY VARIABLES are set to 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So with our model above:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tidy(mod)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)  38.5       3.34     11.5    6.14e-12
## 2 wt           -3.18      0.774    -4.11   3.26e- 4
## 3 cyl          -0.905     0.679    -1.33   1.94e- 1
## 4 hp           -0.0179    0.0122   -1.46   1.56e- 1
## 5 vs            0.155     1.62      0.0957 9.24e- 1
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;wt&lt;/code&gt; = for every ADDITIONAL TON OF WEIGHT a car has, there is an associated &lt;strong&gt;3.18&lt;/strong&gt; decrease in expected MILES PER GALLON (continuous)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cyl&lt;/code&gt; = for every ADDITIONAL ENGINE CYLYNDER a car has, there is an associated &lt;strong&gt;.905&lt;/strong&gt; decrease in expected MILES PER GALLON (continuous)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hp&lt;/code&gt; = for every ADDITIONAL UNIT OF HORSE POWER a car has, there is an associated &lt;strong&gt;.018&lt;/strong&gt; decrease in expected MILES PER GALLON (continuous)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vs&lt;/code&gt; = cars with STRAIGHT ENGINES have, on average, .155 more MILES PER GALLON than cars with V-SHAPED ENGINES. (categorical)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;intercept&lt;/code&gt; = The average MILES PER GALLON of a car with &lt;em&gt;0 weight (wt = 0)&lt;/em&gt;, &lt;em&gt;0 cylinders (cyl = 0)&lt;/em&gt;, &lt;em&gt;0 horsepower (hp = 0)&lt;/em&gt;, and &lt;em&gt;a v-shaped engine (vs = 0)&lt;/em&gt; is &lt;strong&gt;38.5&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s a different example, using the &lt;code&gt;evals&lt;/code&gt; dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# fit model
mod_evals = lm(score ~ age + bty_avg + gender + rank, data = evals)
tidy(mod_evals)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 5
##   term             estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)       4.32      0.195       22.1  3.42e-74
## 2 age              -0.00840   0.00314     -2.67 7.78e- 3
## 3 bty_avg           0.0624    0.0168       3.72 2.22e- 4
## 4 gendermale        0.209     0.0522       4.01 7.07e- 5
## 5 ranktenure track -0.226     0.0804      -2.81 5.13e- 3
## 6 ranktenured      -0.153     0.0622      -2.46 1.42e- 2
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt; = for every ADDITIONAL YEAR OF AGE of a professor, there is an associated &lt;strong&gt;.008&lt;/strong&gt; decrease in their STUDENT EVALUATION SCORE (continuous)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bty_score&lt;/code&gt; = for every ADDITIONAL POINT OF of a professor’s BEAUTY SCORE, there is an associated &lt;strong&gt;.008&lt;/strong&gt; decrease in their STUDENT EVALUATION SCORE (continuous)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gendermale&lt;/code&gt; = MALE professors have, on average, .2 more points on their STUDENT EVALUATION SCORE than FEMALE professors (continuous)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ranktenure track&lt;/code&gt; = TENURE TRACK professors have, on average, .226 fewer points on their STUDENT EVALUATION SCORE than TEACHING (the baseline!) professors (categorical)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ranktenured&lt;/code&gt; = TENURED professors have, on average, .153 fewer points on their STUDENT EVALUATION SCORE than TEACHING (the baseline!) professors (categorical)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;intercept&lt;/code&gt; = the average STUDENT EVALUATION SCORE for a professor who is &lt;em&gt;0 years old (age = 0)&lt;/em&gt;, has a &lt;em&gt;beauty score of 0 (bty_avg = 0)&lt;/em&gt;, is &lt;em&gt;female (gendermale = 0)&lt;/em&gt;, is &lt;em&gt;not tenure track (ranktenure track = 0)&lt;/em&gt; and &lt;em&gt;not tenured (ranktenured = 0)&lt;/em&gt; (i.e., they are “teaching track”, the baseline category)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>R, Rstudio, basics</title>
      <link>/class/getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/getting-started/</guid>
      <description>&lt;h2 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h2&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/files/eat-cake.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;UN Voting Data&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-un-voting-patterns&#34;&gt;Example: UN voting patterns&lt;/h2&gt;
&lt;p&gt;Your first class script &lt;code&gt;eat-cake.R&lt;/code&gt; does two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;takes a dataset that records every vote at the UN going back decades and cleans it&lt;/li&gt;
&lt;li&gt;visualizes voting patterns over time for the US and Turkey&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point, none of this should “make sense” to you. You do not need to understand what each line of code here does. I just want you to run the code and try to make sense of what the different parts are doing.&lt;/p&gt;
&lt;p&gt;Let’s break the script down:&lt;/p&gt;
&lt;p&gt;The first thing the script does is load a set of packages, or libraries, which will give us access to the UN data and functions to use with that data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(unvotes) # this is where the UN data lives
library(lubridate)
library(scales)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see some of the UN data below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;un_votes %&amp;gt;%
  slice(1:5) %&amp;gt;%
  knitr::kable()
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;rcid&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;country&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;country_code&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;vote&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;United States&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;US&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Canada&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CA&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cuba&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CU&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Haiti&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HT&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Dominican Republic&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DO&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The variable &lt;code&gt;rcid&lt;/code&gt; is the way the data identifies the issue being voted on. So on RCID 3, the US voted “yes” while Canada voted “no”.&lt;/p&gt;
&lt;p&gt;Here are some of the issues:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;un_roll_calls %&amp;gt;%
  select(rcid, date, short) %&amp;gt;%
  slice(1:5) %&amp;gt;%
  knitr::kable()
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right&#34;&gt;rcid&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;date&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;short&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1946-01-01&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AMENDMENTS, RULES OF PROCEDURE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1946-01-02&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;SECURITY COUNCIL ELECTIONS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1946-01-04&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VOTING PROCEDURE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1946-01-04&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;DECLARATION OF HUMAN RIGHTS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1946-01-02&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GENERAL ASSEMBLY ELECTIONS&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The next chunk of code takes the UN voting data and calculates the percentage of times the US and Turkey voted “yes” on an issue in each year for which there is data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;un_yes = un_votes %&amp;gt;%
  filter(country %in% c(&amp;quot;United States&amp;quot;, &amp;quot;Turkey&amp;quot;)) %&amp;gt;%
  inner_join(un_roll_calls, by = &amp;quot;rcid&amp;quot;) %&amp;gt;%
  inner_join(un_roll_call_issues, by = &amp;quot;rcid&amp;quot;) %&amp;gt;%
  group_by(country, year = year(date), issue) %&amp;gt;%
  summarize(
    votes = n(),
    percent_yes = mean(vote == &amp;quot;yes&amp;quot;)
  ) %&amp;gt;%
  filter(votes &amp;gt; 5)  # only use records where there are more than 5 votes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the results of this below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;un_yes %&amp;gt;%
  head() %&amp;gt;%
  knitr::kable()
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;country&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;year&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;issue&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;votes&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;percent_yes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Turkey&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1946&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Colonialism&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;15&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.8000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Turkey&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1946&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Economic development&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.6000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Turkey&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1947&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Colonialism&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.2222222&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Turkey&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1947&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Palestinian conflict&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1428571&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Turkey&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1948&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Colonialism&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;12&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.4166667&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Turkey&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1948&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Arms control and disarmament&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So in 1946 Turkey voted 15 times on issues related to “Colonialism”, and of those votes, 80% were a “yes”.&lt;/p&gt;
&lt;p&gt;This last bit of code produces the visualization:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(un_yes, aes(x = year, y = percent_yes, color = country)) +
  geom_point() +
  geom_smooth(method = &amp;quot;loess&amp;quot;, se = FALSE) +
  facet_wrap(~ issue) +
  labs(
    title = &amp;quot;Percentage of &#39;Yes&#39; votes in the UN General Assembly&amp;quot;,
    subtitle = &amp;quot;1946 to 2015&amp;quot;,
    y = &amp;quot;% Yes&amp;quot;,
    x = &amp;quot;Year&amp;quot;,
    color = &amp;quot;Country&amp;quot;
  ) +
  scale_y_continuous(labels = percent)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/getting-started_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Try playing around with the code! What happens when you replace the United States and/or Turkey with another country?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Regression discontinuity</title>
      <link>/class/rdd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/rdd/</guid>
      <description>&lt;h2 id=&#34;regression-discontinuity-design-rdd&#34;&gt;Regression discontinuity design (RDD)&lt;/h2&gt;
&lt;p&gt;Remember the example from class about the effect of being in a gifted program on future earnings. The DAG looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggdag)
library(tidyverse)

# dag
dagify(earnings ~ gifted + ability, 
       gifted ~ ability, 
       exposure = &amp;quot;gifted&amp;quot;, 
       outcome = &amp;quot;earnings&amp;quot;) %&amp;gt;% 
  ggdag_status(text = FALSE, use_labels = &amp;quot;name&amp;quot;) + theme_dag() + 
  theme(legend.position = &amp;quot;none&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/rdd_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;The problem is that students aren’t randomly placed in gifted programs; they are put there for a host of reasons (including, for instance, “ability”).&lt;/p&gt;
&lt;p&gt;The idea behind the RDD is to exploit the fact that the exact test score that gets you into gifted is arbitrary, while the test itself is noisy. This means that students just below and above that cutoff score (75) are pretty similar, but some just barely made it and others just barely didn’t (for reasons that are arguably random: luck, mostly).&lt;/p&gt;
&lt;p&gt;That phrase is key: “for reasons that are arguably &lt;strong&gt;random&lt;/strong&gt;…”. Some students got treatment and others got the control condition randomly, i.e., an experiment.&lt;/p&gt;
&lt;p&gt;Make up data to see how it works:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(1991)


# discontinuity example
fake = tibble(kids = 1:1000, 
       test = runif(1000)*100) %&amp;gt;% 
  mutate(gifted = ifelse(test &amp;gt;= 75, 1, 0)) %&amp;gt;% 
  mutate(earnings = runif(1000)*40 + 10*gifted + .5*test)


ggplot(fake, aes(x = test, y = earnings)) + 
  geom_point()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/rdd_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Do it by hand:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# rdd by hand
fake %&amp;gt;% 
  filter(test &amp;gt;= 73, test &amp;lt;= 77) %&amp;gt;% 
  group_by(gifted) %&amp;gt;% 
  summarise(earnings = mean(earnings))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 2
##   gifted earnings
##    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1      0     54.7
## 2      1     68.2
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Summarizing Data</title>
      <link>/class/data-summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/data-summary/</guid>
      <description>&lt;h2 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h2&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;/slides/code/day1-summarise.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day1-summarise.R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;/slides/code/day2-summarise.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day2-summarise.R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summarize&#34;&gt;Summarize&lt;/h2&gt;
&lt;p&gt;Let’s load the libraries.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# libraries
library(tidyverse)
library(nycflights13)
library(fivethirtyeight)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Say we want to take the average of a variable in our dataset. &lt;code&gt;summarize()&lt;/code&gt; can help us do that.&lt;/p&gt;
&lt;p&gt;Say we wanted to know how late in departure is the &lt;em&gt;average&lt;/em&gt; flight in our dataset and what’s the latest a flight has ever been?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## on average, how late are flights in departing?
flights %&amp;gt;%
  summarise(avg_late = mean(dep_delay, na.rm = TRUE),
            most_late = max(dep_delay, na.rm = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   avg_late most_late
##      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     12.6      1301
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not the &lt;code&gt;na.rm = TRUE&lt;/code&gt; above and what happens if you remove it. The problem is there are missing values (&lt;code&gt;NA&lt;/code&gt;) in our data, and R can’t take the average of a bunch of numbers where some are missing. &lt;code&gt;na.rm = TRUE&lt;/code&gt; tells R to ignore those missing numbers and use only the complete observations.&lt;/p&gt;
&lt;h2 id=&#34;summarize--group_by&#34;&gt;Summarize + group_by()&lt;/h2&gt;
&lt;p&gt;Say we wanted to know how average departure delays vary across airlines. Conceptually, this means taking the average of departure delays for each airline in the dataset separately. We can do this by combining &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;summarise()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# what if we wanted to know these statistics
## for each month in our dataset?
carrier_late = flights %&amp;gt;%
  group_by(carrier) %&amp;gt;%
  summarise(avg_late = mean(dep_delay, na.rm = TRUE),
            most_late = max(dep_delay, na.rm = TRUE))


# make a plot
ggplot(carrier_late, aes(x = carrier, y = avg_late)) +
  geom_col() +
  coord_flip()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-summary_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;the-bob-ross-example&#34;&gt;The Bob Ross example&lt;/h2&gt;
&lt;p&gt;Happy tree?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bob_ross %&amp;gt;%
  summarise(prop_tree = mean(tree, na.rm = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 1
##   prop_tree
##       &amp;lt;dbl&amp;gt;
## 1     0.896
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clouds over time?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bob_clouds = bob_ross %&amp;gt;%
  group_by(season) %&amp;gt;%
  summarise(prop_clouds = mean(clouds, na.rm = TRUE))

ggplot(bob_clouds, aes(x = season, y = prop_clouds)) + geom_line()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-summary_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;snowy mountain?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bob_ross %&amp;gt;%
  filter(mountain == 1) %&amp;gt;%
  summarise(snowiness = mean(snowy_mountain, na.rm = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 1
##   snowiness
##       &amp;lt;dbl&amp;gt;
## 1     0.681
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bob_ross %&amp;gt;%
  group_by(mountain) %&amp;gt;%
  summarise(snowiness = mean(snowy_mountain, na.rm = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 2
##   mountain snowiness
##      &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
## 1        0     0    
## 2        1     0.681
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Steve ross?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;bob_ross %&amp;gt;%
  group_by(steve_ross) %&amp;gt;%
  summarise(lake_chance = mean(lake, na.rm = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 2
##   steve_ross lake_chance
##        &amp;lt;int&amp;gt;       &amp;lt;dbl&amp;gt;
## 1          0       0.339
## 2          1       0.909
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-flying-etiquette-example&#34;&gt;The flying etiquette example&lt;/h2&gt;
&lt;p&gt;Middle arm rest?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;middle_arm_rests = flying %&amp;gt;%
  group_by(two_arm_rests) %&amp;gt;%
  tally() %&amp;gt;%
  mutate(percent = n/sum(n))

ggplot(middle_arm_rests, aes(x = percent, y = two_arm_rests)) +
  geom_col()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-summary_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Unruly children?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;nasty_kids = flying %&amp;gt;%
  group_by(children_under_18, unruly_child) %&amp;gt;%
  tally() %&amp;gt;%
  mutate(p_unruly = n/sum(n))

ggplot(nasty_kids, aes(x = unruly_child, y = p_unruly, fill = children_under_18)) + geom_col(position = &amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-summary_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
</description>
    </item>
    
    <item>
      <title>Tidy data</title>
      <link>/class/data-tidy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/data-tidy/</guid>
      <description>&lt;h2 id=&#34;tidy-data&#34;&gt;Tidy data&lt;/h2&gt;
&lt;p&gt;We can use the &lt;code&gt;pivot_longer&lt;/code&gt; function to make data that is in “wide” format into “long” format.&lt;/p&gt;
&lt;p&gt;Here’s an example, using the &lt;code&gt;drinks&lt;/code&gt; dataset from &lt;code&gt;fivethirtyheight&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# load libraries
library(tidyverse)
library(fivethirtyeight)

# too many countries, let&#39;s look at a few
# %in% is a new logical operator: returns observations that match one of the strings
drinks_subset = 
  drinks %&amp;gt;% 
  filter(country %in% c(&amp;quot;USA&amp;quot;, &amp;quot;China&amp;quot;, &amp;quot;Italy&amp;quot;, &amp;quot;Saudi Arabia&amp;quot;)) 


# let&#39;s gather the three alcohol variables into two: type and serving
tidy_drinks = drinks_subset %&amp;gt;% 
  pivot_longer(cols = c(beer_servings, spirit_servings, wine_servings), 
               names_to = &amp;quot;type&amp;quot;, values_to = &amp;quot;serving&amp;quot;)
tidy_drinks
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 × 4
##    country      total_litres_of_pure_alcohol type            serving
##    &amp;lt;chr&amp;gt;                               &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt;
##  1 China                                 5   beer_servings        79
##  2 China                                 5   spirit_servings     192
##  3 China                                 5   wine_servings         8
##  4 Italy                                 6.5 beer_servings        85
##  5 Italy                                 6.5 spirit_servings      42
##  6 Italy                                 6.5 wine_servings       237
##  7 Saudi Arabia                          0.1 beer_servings         0
##  8 Saudi Arabia                          0.1 spirit_servings       5
##  9 Saudi Arabia                          0.1 wine_servings         0
## 10 USA                                   8.7 beer_servings       249
## 11 USA                                   8.7 spirit_servings     158
## 12 USA                                   8.7 wine_servings        84
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# let&#39;s put position = dodge in geom_col, which will place bars side by side
ggplot(tidy_drinks, aes(x = country, y = serving, fill = type)) + 
  geom_col(position = &amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-tidy_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Here’s another example, using the &lt;code&gt;masculinity survey&lt;/code&gt; from &lt;code&gt;fivethirtyeight&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# different dataset on masculinity
masculinity_survey
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 189 × 12
##    question  response overall age_18_34 age_35_64 age_65_over white_yes white_no
##    &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 &amp;quot;In gene… Very ma…    0.37      0.29      0.42        0.37      0.34     0.44
##  2 &amp;quot;In gene… Somewha…    0.46      0.47      0.46        0.47      0.5      0.39
##  3 &amp;quot;In gene… Not ver…    0.11      0.13      0.09        0.13      0.11     0.11
##  4 &amp;quot;In gene… Not at …    0.05      0.1       0.02        0.03      0.04     0.06
##  5 &amp;quot;In gene… No answ…    0.01      0         0.01        0.01      0.01     0   
##  6 &amp;quot;How imp… Very im…    0.16      0.18      0.17        0.13      0.11     0.26
##  7 &amp;quot;How imp… Somewha…    0.37      0.38      0.37        0.32      0.38     0.35
##  8 &amp;quot;How imp… Not too…    0.28      0.18      0.31        0.37      0.32     0.2 
##  9 &amp;quot;How imp… Not at …    0.18      0.26      0.15        0.18      0.18     0.19
## 10 &amp;quot;How imp… No answ…    0         0         0.01        0         0        0   
## # ℹ 179 more rows
## # ℹ 4 more variables: children_yes &amp;lt;dbl&amp;gt;, children_no &amp;lt;dbl&amp;gt;,
## #   straight_yes &amp;lt;dbl&amp;gt;, straight_no &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# focus on one question
# collapse age categories into long format
manly_pressure = masculinity_survey %&amp;gt;% 
  filter(question == &amp;quot;Do you think that society puts pressure on men in a way that is unhealthy or bad for them?&amp;quot;) %&amp;gt;% 
  pivot_longer(names_to = &amp;quot;ages&amp;quot;, 
               values_to = &amp;quot;percent&amp;quot;, 
               c(age_18_34, age_35_64, age_65_over))

manly_pressure
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 × 11
##   question          response overall white_yes white_no children_yes children_no
##   &amp;lt;fct&amp;gt;             &amp;lt;fct&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 Do you think tha… Yes         0.6       0.58     0.65         0.56        0.66
## 2 Do you think tha… Yes         0.6       0.58     0.65         0.56        0.66
## 3 Do you think tha… Yes         0.6       0.58     0.65         0.56        0.66
## 4 Do you think tha… No          0.39      0.41     0.35         0.44        0.34
## 5 Do you think tha… No          0.39      0.41     0.35         0.44        0.34
## 6 Do you think tha… No          0.39      0.41     0.35         0.44        0.34
## 7 Do you think tha… No answ…    0.01      0.01     0            0.01        0   
## 8 Do you think tha… No answ…    0.01      0.01     0            0.01        0   
## 9 Do you think tha… No answ…    0.01      0.01     0            0.01        0   
## # ℹ 4 more variables: straight_yes &amp;lt;dbl&amp;gt;, straight_no &amp;lt;dbl&amp;gt;, ages &amp;lt;chr&amp;gt;,
## #   percent &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can plot the results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot
ggplot(manly_pressure, aes(x = response, y = percent, fill = ages)) + 
  geom_col(position = &amp;quot;dodge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-tidy_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Finally, here’s another example using &lt;code&gt;relig_income&lt;/code&gt;. Notice here how instead of explicitly writing out every variable we want to collapse, we can just exclude the only other variable in the dataset via the “-”.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# look at the data
relig_income
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18 × 11
##    religion `&amp;lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`
##    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1 Agnostic      27        34        60        81        76       137        122
##  2 Atheist       12        27        37        52        35        70         73
##  3 Buddhist      27        21        30        34        33        58         62
##  4 Catholic     418       617       732       670       638      1116        949
##  5 Don’t k…      15        14        15        11        10        35         21
##  6 Evangel…     575       869      1064       982       881      1486        949
##  7 Hindu          1         9         7         9        11        34         47
##  8 Histori…     228       244       236       238       197       223        131
##  9 Jehovah…      20        27        24        24        21        30         15
## 10 Jewish        19        19        25        25        30        95         69
## 11 Mainlin…     289       495       619       655       651      1107        939
## 12 Mormon        29        40        48        51        56       112         85
## 13 Muslim         6         7         9        10         9        23         16
## 14 Orthodox      13        17        23        32        32        47         38
## 15 Other C…       9         7        11        13        13        14         18
## 16 Other F…      20        33        40        46        49        63         46
## 17 Other W…       5         2         3         4         2         7          3
## 18 Unaffil…     217       299       374       365       341       528        407
## # ℹ 3 more variables: `$100-150k` &amp;lt;dbl&amp;gt;, `&amp;gt;150k` &amp;lt;dbl&amp;gt;,
## #   `Don&#39;t know/refused` &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# make tidy
tidy_relig = relig_income %&amp;gt;% 
  pivot_longer(-religion, names_to = &amp;quot;income_categories&amp;quot;, 
               values_to = &amp;quot;responses&amp;quot;) %&amp;gt;% 
  group_by(religion) %&amp;gt;% 
  mutate(percent = responses/sum(responses))


# make the plot
ggplot(tidy_relig, aes(x = income_categories, y = percent)) + 
  geom_col() + 
  facet_wrap(vars(religion)) + 
  coord_flip() + 
  theme_light()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-tidy_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;counts-and-percentages-group_by--tally&#34;&gt;Counts and percentages (group_by + tally)&lt;/h2&gt;
&lt;p&gt;Say we wanted to count how many characters in the starwars dataset have blonde, brown, etc., hair. I can do this with &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;tally&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;starwars %&amp;gt;% 
  group_by(hair_color) %&amp;gt;% 
  tally()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 13 × 2
##    hair_color        n
##    &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt;
##  1 auburn            1
##  2 auburn, grey      1
##  3 auburn, white     1
##  4 black            13
##  5 blond             3
##  6 blonde            1
##  7 brown            18
##  8 brown, grey       1
##  9 grey              1
## 10 none             37
## 11 unknown           1
## 12 white             4
## 13 &amp;lt;NA&amp;gt;              5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, with &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarise&lt;/code&gt; and &lt;code&gt;n()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;starwars %&amp;gt;% 
  group_by(hair_color) %&amp;gt;% 
  summarise(n = n())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 13 × 2
##    hair_color        n
##    &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt;
##  1 auburn            1
##  2 auburn, grey      1
##  3 auburn, white     1
##  4 black            13
##  5 blond             3
##  6 blonde            1
##  7 brown            18
##  8 brown, grey       1
##  9 grey              1
## 10 none             37
## 11 unknown           1
## 12 white             4
## 13 &amp;lt;NA&amp;gt;              5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, say I wanted to calculate the &lt;em&gt;percent&lt;/em&gt; of characters with each eye color. I can do this below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;starwars %&amp;gt;% 
  group_by(hair_color) %&amp;gt;% 
  tally() %&amp;gt;% 
  mutate(percent = n/sum(n))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 13 × 3
##    hair_color        n percent
##    &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 auburn            1  0.0115
##  2 auburn, grey      1  0.0115
##  3 auburn, white     1  0.0115
##  4 black            13  0.149 
##  5 blond             3  0.0345
##  6 blonde            1  0.0115
##  7 brown            18  0.207 
##  8 brown, grey       1  0.0115
##  9 grey              1  0.0115
## 10 none             37  0.425 
## 11 unknown           1  0.0115
## 12 white             4  0.0460
## 13 &amp;lt;NA&amp;gt;              5  0.0575
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;factor-variables&#34;&gt;Factor variables&lt;/h2&gt;
&lt;p&gt;Sometimes we have a categorical variable (e.g., months of the year) that we understand as having some qualitative ordering (e.g., January comes before June). R doesn’t know this though, but we can tell it using &lt;code&gt;factor&lt;/code&gt; variables.&lt;/p&gt;
&lt;p&gt;Here’s an example using some data I made up:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# i have data on weather that looks like this:
weather = tibble(temp = c(80, 23, 14, 23, 25), 
                 month = c(&amp;quot;January&amp;quot;, &amp;quot;December&amp;quot;, 
                           &amp;quot;July&amp;quot;, &amp;quot;June&amp;quot;, &amp;quot;October&amp;quot;))

weather
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 2
##    temp month   
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   
## 1    80 January 
## 2    23 December
## 3    14 July    
## 4    23 June    
## 5    25 October
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# i want the month variable in order
# i can use factors for this
weather_factor = weather %&amp;gt;% 
  mutate(month_factor = factor(month, levels = c(&amp;quot;January&amp;quot;, &amp;quot;June&amp;quot;, 
                                          &amp;quot;July&amp;quot;, &amp;quot;October&amp;quot;, 
                                          &amp;quot;December&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice plot without factor:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(weather, aes(x = month, y = temp)) + geom_col()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-tidy_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;And new and imrpoved plot where month is a factor:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(weather_factor, aes(x = month_factor, y = temp)) + geom_col()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-tidy_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h3 id=&#34;fct_reorder&#34;&gt;fct_reorder&lt;/h3&gt;
&lt;p&gt;Instead of explicitly telling R how we want to order a factor, we can instead use another variable in the dataset to determine the order.&lt;/p&gt;
&lt;p&gt;Look at the example below, using the &lt;code&gt;starwars&lt;/code&gt; dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# starwars example
starwars
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 87 × 14
##    name     height  mass hair_color skin_color eye_color birth_year sex   gender
##    &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; 
##  1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…
##  2 C-3PO       167    75 &amp;lt;NA&amp;gt;       gold       yellow         112   none  mascu…
##  3 R2-D2        96    32 &amp;lt;NA&amp;gt;       white, bl… red             33   none  mascu…
##  4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…
##  5 Leia Or…    150    49 brown      light      brown           19   fema… femin…
##  6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…
##  7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…
##  8 R5-D4        97    32 &amp;lt;NA&amp;gt;       white, red red             NA   none  mascu…
##  9 Biggs D…    183    84 black      light      brown           24   male  mascu…
## 10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…
## # ℹ 77 more rows
## # ℹ 5 more variables: homeworld &amp;lt;chr&amp;gt;, species &amp;lt;chr&amp;gt;, films &amp;lt;list&amp;gt;,
## #   vehicles &amp;lt;list&amp;gt;, starships &amp;lt;list&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# count how many characters with each eye_color
star_eyes = starwars %&amp;gt;% 
  group_by(eye_color) %&amp;gt;% 
  tally()

star_eyes = star_eyes %&amp;gt;% 
  mutate(eye_color = fct_reorder(eye_color, n))

ggplot(star_eyes, aes(x = eye_color, y = n)) + 
  geom_col() + 
  coord_flip()
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/data-tidy_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;
</description>
    </item>
    
    <item>
      <title>Uncertainty</title>
      <link>/class/uncertainty/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/class/uncertainty/</guid>
      <description>&lt;h1 id=&#34;in-class-example&#34;&gt;In-class example&lt;/h1&gt;
&lt;p&gt;Here’s the code we’ll be using in class. Download it and store it with the rest of your materials for this course. If simply clicking doesn’t trigger download, you should right-click and select “save link as…”.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/slides/code/day1-uncertainty.R&#34;&gt;&lt;i class=&#34;fas fa-file-archive&#34;&gt;&lt;/i&gt; &lt;code&gt;day1-uncertainty.R&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;other-examples&#34;&gt;Other examples&lt;/h1&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(broom)
library(socviz)
library(moderndive)

set.seed(1990)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;why-are-we-uncertain&#34;&gt;Why are we uncertain?&lt;/h2&gt;
&lt;p&gt;Remember that with data we are always trying to estimate something: the effect of X on Y, the average value of Y among some population, etc, – but we can be more or less &lt;em&gt;certain&lt;/em&gt; about our estimate.&lt;/p&gt;
&lt;p&gt;A key reason we are uncertain is that our data is usually a &lt;em&gt;sample&lt;/em&gt; of something bigger that we want to learn about (that bigger thing = the &lt;em&gt;population&lt;/em&gt;). The problem is that each sample is going to give us a (slightly) different answer.&lt;/p&gt;
&lt;p&gt;For example, below, imagine that in the US there were only &lt;code&gt;\(\approx\)&lt;/code&gt; 2,800 people, and they were all included in the &lt;code&gt;gss_sm&lt;/code&gt; dataset from &lt;code&gt;socviz&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What proportion of men and women voted for Obama in 2012? In this made up example, we can know the &lt;em&gt;exact answer&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;true_prop = gss_sm %&amp;gt;% 
  group_by(sex) %&amp;gt;% 
  summarise(obama = mean(obama, na.rm = TRUE))
true_prop
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 2
##   sex    obama
##   &amp;lt;fct&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 Male   0.576
## 2 Female 0.663
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, imagine that we instead only had a &lt;em&gt;sample&lt;/em&gt; of &lt;code&gt;gss_sm.&lt;/code&gt; Say, we picked 10 random people from &lt;code&gt;gss_sm&lt;/code&gt; and calculated the proportion of support among men and women for Obama.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;% 
  # randomly draw 10 observations
  sample_n(10) %&amp;gt;% 
  group_by(sex) %&amp;gt;% 
  summarise(obama = mean(obama, na.rm = TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 2
##   sex    obama
##   &amp;lt;fct&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 Male    0.25
## 2 Female  1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the code above over and over. Notice: you get a different answer each time! Some very far from the true proportion of men and women who voted from Obama. This is the big problem: we only have a sample and each sample gives us a different answer.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; How do we know that our sample is &lt;em&gt;close&lt;/em&gt; to the true answer and not weirdly off?&lt;/p&gt;
&lt;h2 id=&#34;become-less-uncertain&#34;&gt;Become less uncertain&lt;/h2&gt;
&lt;p&gt;It turns out that if a sample is &lt;em&gt;random, representative, and large&lt;/em&gt;, our estimate from the &lt;strong&gt;sample&lt;/strong&gt; will be pretty close to the estimate from the population.&lt;/p&gt;
&lt;p&gt;Let’s see how this works: the code below uses the function &lt;code&gt;rep_sample_n&lt;/code&gt; from &lt;code&gt;moderndive&lt;/code&gt; to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pick &lt;code&gt;N&lt;/code&gt; random people from &lt;code&gt;gss_sm&lt;/code&gt; (our sample)&lt;/li&gt;
&lt;li&gt;Estimate the proportion of men and women who voted for Obama in that sample (our sample estimate)&lt;/li&gt;
&lt;li&gt;Repeat this &lt;code&gt;P&lt;/code&gt; times&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below, &lt;code&gt;N&lt;/code&gt; = 10 and &lt;code&gt;P&lt;/code&gt; = 10. So 10 different samples, each with 10 random people.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;% 
  # 10 samples, each of size 10
  rep_sample_n(size = 10, reps = 10) %&amp;gt;% 
  group_by(sex, replicate) %&amp;gt;% 
  summarise(obama = mean(obama, na.rm = TRUE)) %&amp;gt;% 
  arrange(replicate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 19 × 3
## # Groups:   sex [2]
##    sex    replicate   obama
##    &amp;lt;fct&amp;gt;      &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 Female         1   0.571
##  2 Male           2 NaN    
##  3 Female         2   0.333
##  4 Male           3   0    
##  5 Female         3   0.5  
##  6 Male           4   0.5  
##  7 Female         4   0.5  
##  8 Male           5   0    
##  9 Female         5   1    
## 10 Male           6   0.5  
## 11 Female         6   0.6  
## 12 Male           7   0    
## 13 Female         7   1    
## 14 Male           8   0.667
## 15 Female         8   0.5  
## 16 Male           9   1    
## 17 Female         9   1    
## 18 Male          10   0.5  
## 19 Female        10   0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how the sample estimate differ varies across samples (the &lt;code&gt;replicate&lt;/code&gt; variable tells you which sample you are looking at).&lt;/p&gt;
&lt;p&gt;Let’s do this many more times, and plot the distribution of our sample estimates. Below, each sample only has 10 people in it, and we take 1,000 samples. The black bars are the &lt;em&gt;proportion of men and women who voted for Obama across all of gss_sm&lt;/em&gt; (the population parameter):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;% 
  rep_sample_n(size = 10, reps = 1000) %&amp;gt;% 
  group_by(replicate, sex) %&amp;gt;% 
  summarise(obama = mean(obama, na.rm = TRUE)) %&amp;gt;% 
  ggplot(aes(x = obama, fill = sex)) + 
  geom_histogram(color = &amp;quot;white&amp;quot;, alpha = .8) + 
  facet_wrap(vars(sex), scales = &amp;quot;free&amp;quot;) + 
  theme_bw() + 
  geom_vline(data = true_prop, aes(xintercept = obama), 
             size = 2, color = &amp;quot;black&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/uncertainty_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;This is a sample that is &lt;em&gt;random and representative&lt;/em&gt;, but small (only 10 people in each sample!). Notice how some of our sample estimates are far off from the population estimate: we get lots of cases where every man, for instance, voted for Obama, which is really wrong.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;% 
  rep_sample_n(size = 10, reps = 1000) %&amp;gt;% 
  group_by(replicate, sex) %&amp;gt;% 
  summarise(obama = mean(obama, na.rm = TRUE)) %&amp;gt;% 
  ggplot(aes(x = obama, fill = sex)) + 
  geom_histogram(color = &amp;quot;white&amp;quot;, alpha = .8) + 
  facet_wrap(vars(sex), scales = &amp;quot;free&amp;quot;) + 
  theme_bw() + 
  geom_vline(data = true_prop, aes(xintercept = obama), 
             size = 2, color = &amp;quot;black&amp;quot;) + 
  scale_x_continuous(limits = c(-.1, 1.1))
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/uncertainty_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Now look what happens when we make our samples bigger, for instance, where each sample is composed of 50 people:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;% 
  rep_sample_n(size = 50, reps = 1000) %&amp;gt;% 
  group_by(replicate, sex) %&amp;gt;% 
  summarise(obama = mean(obama, na.rm = TRUE)) %&amp;gt;% 
  ggplot(aes(x = obama, fill = sex)) + 
  geom_histogram(color = &amp;quot;white&amp;quot;, alpha = .8) + 
  facet_wrap(vars(sex), scales = &amp;quot;free&amp;quot;) + 
  theme_bw() + 
  geom_vline(data = true_prop, aes(xintercept = obama), 
             size = 2, color = &amp;quot;black&amp;quot;)  + 
  scale_x_continuous(limits = c(0, 1)) + 
  scale_x_continuous(limits = c(-.1, 1.1))
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/uncertainty_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Now our estimates look very different: many of them are pretty close to the true population proportion. Let’s make our samples even bigger, with 100 people each:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;% 
  rep_sample_n(size = 100, reps = 1000) %&amp;gt;% 
  group_by(replicate, sex) %&amp;gt;% 
  summarise(obama = mean(obama, na.rm = TRUE)) %&amp;gt;% 
  ggplot(aes(x = obama, fill = sex)) + 
  geom_histogram(color = &amp;quot;white&amp;quot;, alpha = .8) + 
  facet_wrap(vars(sex), scales = &amp;quot;free&amp;quot;) + 
  theme_bw() + 
  geom_vline(data = true_prop, aes(xintercept = obama), 
             size = 2, color = &amp;quot;black&amp;quot;)  + 
  scale_x_continuous(limits = c(0, 1)) + 
  scale_x_continuous(limits = c(-.1, 1.1))
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/uncertainty_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Notice how much “narrower” the distribution is getting. We’re getting fewer and fewer samples that are far away from the black line. Let’s look at a sample of 500:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;% 
  rep_sample_n(size = 500, reps = 1000) %&amp;gt;% 
  group_by(replicate, sex) %&amp;gt;% 
  summarise(obama = mean(obama, na.rm = TRUE)) %&amp;gt;% 
  ggplot(aes(x = obama, fill = sex)) + 
  geom_histogram(color = &amp;quot;white&amp;quot;, alpha = .8) + 
  facet_wrap(vars(sex), scales = &amp;quot;free&amp;quot;) + 
  theme_bw() + 
  geom_vline(data = true_prop, aes(xintercept = obama), 
             size = 2, color = &amp;quot;black&amp;quot;)  + 
  scale_x_continuous(limits = c(0, 1)) + 
  scale_x_continuous(limits = c(-.1, 1.1))
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/uncertainty_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Even closer! The result is that if you take a large, random, representative sample the vast majority of estimates are going to be pretty damn close to the population parameter.&lt;/p&gt;
&lt;h2 id=&#34;more-data-more-certain&#34;&gt;More data, more certain&lt;/h2&gt;
&lt;p&gt;What’s happening above is pretty intuitive; if you are estimating something, you should feel more confident about that estimate the more data you have.&lt;/p&gt;
&lt;p&gt;Compare the two trend-lines below: we should be much more certain about the one on the left, even though both have the sample slope (2)!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# uncertainty example
low_certain = tibble(x = rnorm(10), 
                     y = 2*x + rnorm(10), 
                     certain = &amp;quot;low&amp;quot;)

hi_certain = tibble(x = rnorm(1000), 
                     y = 2*x + rnorm(1000), 
                     certain = &amp;quot;high&amp;quot;)


rbind(low_certain, hi_certain) %&amp;gt;% 
  ggplot(aes(x = x, y = y, color = certain)) + 
  geom_jitter(size = 2, width = 1) + 
  facet_wrap(vars(certain)) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;, color = &amp;quot;blue&amp;quot;) + 
  theme_bw() + 
  theme(legend.position = &amp;quot;none&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/uncertainty_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
&lt;h2 id=&#34;good-and-bad-samples&#34;&gt;Good and bad samples&lt;/h2&gt;
&lt;p&gt;Bigger samples decrease uncertainty in our estimate, but this all hinges on the sample being &lt;em&gt;good&lt;/em&gt;. By good I mean the sample is random and representative of the population.&lt;/p&gt;
&lt;p&gt;What makes a sample representative of the population? It’s easier to think about what makes a sample unrepresentative. Imagine, for whatever reason, that in the example above, &lt;em&gt;no one under 50 showed up in our sample&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We can simulate this easily by just adding a call to &lt;code&gt;filter&lt;/code&gt; below to exclude people below 50:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gss_sm %&amp;gt;% 
  filter(age &amp;gt;= 50) %&amp;gt;% 
  rep_sample_n(size = 500, reps = 1000) %&amp;gt;% 
  group_by(replicate, sex) %&amp;gt;% 
  summarise(obama = mean(obama, na.rm = TRUE)) %&amp;gt;% 
  ggplot(aes(x = obama, fill = sex)) + 
  geom_histogram(color = &amp;quot;white&amp;quot;, alpha = .8) + 
  facet_wrap(vars(sex), scales = &amp;quot;free&amp;quot;) + 
  theme_bw() + 
  geom_vline(data = true_prop, aes(xintercept = obama), 
             size = 2, color = &amp;quot;black&amp;quot;)  + 
  scale_x_continuous(limits = c(0, 1)) + 
  scale_x_continuous(limits = c(-.1, 1.1))
&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/class/uncertainty_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;
&lt;p&gt;Notice how the distribution of sample estimates are no longer centered around the true population parameter (the black lines). They are centered elsewhere. Increasing the sample size will continue to make the distribution taller and skinnier, but it will not be centered around the black lines.&lt;/p&gt;
&lt;p&gt;This is because our sample is &lt;em&gt;biased&lt;/em&gt;. In the population, there are people of all sorts of ages. But in our sample, it’s only older people. A bigger sample will not solve this problem.&lt;/p&gt;
&lt;p&gt;Why would our sample not have young people in it, even though the population does? These &lt;em&gt;sampling biases&lt;/em&gt; happens in the real world all the time. Maybe younger people are less willing to answer polls, or to take surveys, or to have landlines, etc., that means they are “missing” at higher rates from samples.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;There are other sources of uncertainty (e.g., measurement error) which we will not discuss here.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
